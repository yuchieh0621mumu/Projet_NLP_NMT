{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n02MwmAKytqd",
        "outputId": "8ed6f221-7f0b-4e01-8a56-814bdfcfa465"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: Hi, this is Elena and I'm going to be presenting our work, Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling.\n",
            "Target: Bonjour, je m’appelle Elena et je vais vous présenter notre travail, détecter les emprunts non assimilés en espagnol : un corpus annoté et des approches de modélisation.\n",
            "BLEU score: 7.992219124248642e-232\n",
            "\n",
            "Source: \n",
            "Target: So we're going to be covering what lexical borrowing is, the task that we proposed, the dataset that we have released and some models that we explored.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous allons donc couvrir ce qu’est l’emprunt lexical, la tâche que nous avons proposée, les données que nous avons publiées et certains modèles que nous avons explorés.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: But to begin with, what is lexical borrowing and why it matters as an NLP task?\n",
            "Target: Mais pour commencer, qu’est-ce que l’emprunt lexical et pourquoi est-ce important en tant que tâche de TAL traitement automatique du langage naturel ?\n",
            "BLEU score: 8.319100378795605e-232\n",
            "\n",
            "Source: \n",
            "Target: Well, lexical borrowing is basically the incorporation of words from one language into another language.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Eh bien, l’emprunt lexical est fondamentalement l’incorporation de mots d’une langue dans une autre langue.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For instance, in Spanish we use words that come from English.\n",
            "Target: Par exemple, en espagnol, nous utilisons des mots qui viennent de l’anglais.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: And here you have a few examples, words such as podcast, app, and online crowdfunding, all these are English words that we sometimes use in Spanish.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et ici, vous avez quelques exemples : des mots tels que podcast, app et crowdfunding en ligne ; tous ces mots sont des mots anglais que nous utilisons parfois en espagnol.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Lexical borrowing is a type of linguistic borrowing um which is basically reproducing in one language patterns of other languages.\n",
            "Target: L’emprunt lexical est un type d’emprunt linguistique qui consiste essentiellement à reproduire dans une langue des modèles d’autres langues.\n",
            "BLEU score: 8.278703315138962e-232\n",
            "\n",
            "Source: \n",
            "Target: And borrowing and code switching have sometimes been compared and described as a continuum, code switching being ah the thing that bilinguals do where they mix two languages at the same time.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et l’emprunt et l’alternance codique ont parfois été comparés et décrits comme un continuum, l’alternance codique étant la chose que font les bilingues lorsqu’ils mélangent deux langues en même temps.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: There are however some differences between lexical borrowing and code-switching.\n",
            "Target: Il y a cependant quelques différences entre l’emprunt lexical et l’alternance codique.\n",
            "BLEU score: 9.788429383461836e-232\n",
            "\n",
            "Source: \n",
            "Target: We're going to be focusing on lexical borrowing.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous allons nous concentrer sur l’emprunt lexical.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Code switching is something that is done by bilinguals and by definition the code switches are not integrated into any of the languages in use, whereas lexical borrowing is something that is also done by monolinguals.\n",
            "Target: L’alternance codique est quelque chose qui est fait par les bilingues et par définition, les alternances codiques ne sont pas intégrées dans l’une des langues utilisées, alors que l’emprunt lexical est quelque chose qui est également fait par les monolingues.\n",
            "BLEU score: 7.244248269687037e-232\n",
            "\n",
            "Source: \n",
            "Target: The borrowings will comply with the grammar of the recipient language.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les emprunts seront conformes à la grammaire de la langue du destinataire.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And borrowings can eventually be integrated into the recipient language.\n",
            "Target: Et les emprunts peuvent éventuellement être intégrés dans la langue du destinataire.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: So why is borrowing an interesting phenomenon?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Alors pourquoi emprunter un phénomène intéressant ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Well, from the point of view of linguistics, borrowing is a manifestation of of how languages change and how they interact.\n",
            "Target: Eh bien, du point de vue de la linguistique, l’emprunt est une manifestation de la façon dont les langues changent et comment elles interagissent.\n",
            "BLEU score: 9.788429383461836e-232\n",
            "\n",
            "Source: \n",
            "Target: And also lexical borrowings are a source of new words.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et aussi, les emprunts lexicaux sont une source de nouveaux mots.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Here you have some examples of lexical borrowings that have been incorporated into the Spanish language as new words.\n",
            "Target: Ici, vous avez quelques exemples d’emprunts lexicaux qui ont été incorporés dans la langue espagnole en tant que nouveaux mots.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: In terms of NLP ah borrowings are a common source of out-of-vocabulary words.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En termes de TAL traitement automatique du langage naturel, les emprunts sont une source courante de mots hors-vocabulaire.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And in fact, automatically detecting lexical borrowings ah has proven to be useful for NLP downstream tasks such as parsing, text-to-speech synthesis or machine translation.\n",
            "Target: Et en effet, détecter automatiquement les emprunts lexicaux s’est avéré utile pour les tâches downstream de TAL traitement automatique du langage naturel telles que l’analyse syntaxique, la synthèse texte-parole ou la traduction automatique.\n",
            "BLEU score: 7.601159375410181e-232\n",
            "\n",
            "Source: \n",
            "Target: There has been a growing interest in the influence of English on other languages ah particularly ah related to English lexical borrowings, borrowings which sometimes have been called Anglicisms.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il y a eu un intérêt croissant pour l’influence de l’anglais sur d’autres langues, en particulier les emprunts lexicaux anglais, des emprunts qui ont parfois été appelés anglicismes.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And here, you have some examples of ah work on automatic detection of borrowings in ah some of these languages.\n",
            "Target: Et ici, vous avez quelques exemples de travail sur la détection automatique des emprunts dans certaines de ces langues.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: So the task that we propose is to detect unassimilated lexical borrowings in Spanish newswire.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Donc, la tâche que nous proposons est de détecter les emprunts lexicaux non assimilés dans le fil d’actualité espagnol.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Which means that we are interested in extracting ah words borrowed from other languages that are being used in Spanish newspapers but that have not been integrated or assimilated into the recipient language.\n",
            "Target: Ce qui signifie que cela nous intéresse d'extraire les mots empruntés à d’autres langues qui sont utilisés dans les journaux espagnols, mais qui n’ont pas été intégrés ou assimilés dans la langue du destinataire.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: So not yet integrated into Spanish.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Donc pas encore intégrés à l’espagnol.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Here you have an example.\n",
            "Target: Vous avez ici un exemple.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This is a sentence in Spanish: Las prendas bestsellers se estampan con motivos florales, animal print o retales tipo patchwork.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ceci est une phrase en espagnol : Las prendas bestsellers se estampan con motivos florales, animal print o retales tipo patchwork.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Um, and as you can see, there are three spans of texts which are actually English words like bestseller, animal print and patchwork.\n",
            "Target: Hum, et comme vous pouvez le voir, il y a trois étendages de textes qui sont en réalité des mots anglais comme bestseller, animal print et patchwork.\n",
            "BLEU score: 1.7845935722073316e-78\n",
            "\n",
            "Source: \n",
            "Target: These are the type of spans that we are interested in extracting and detecting.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il s’agit du type d’étendages qu'il nous intéresse d'extraire et de détecter.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: There has been previous word on Anglicism detection ah which consists consisted of a CRF model for Anglicism detection on Spanish Newswire.\n",
            "Target: Il y a eu un mot antérieur sur la détection de l’anglicisme qui consiste en un modèle CRF pour la détection de l’anglicisme sur le fil d’actualité espagnol.\n",
            "BLEU score: 9.418382295637229e-232\n",
            "\n",
            "Source: \n",
            "Target: This model achieved an F1 score of eighty six.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ce modèle a obtenu un score F1 de quatre-vingt-six.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: But there were some limitations both um in the dataset and the modeling approach.\n",
            "Target: Mais il y avait des limites à la fois dans les données et dans l’approche de modélisation.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: So the dataset focused exclusively on one source of news, consisted only of headlines.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ainsi, les données se concentraient exclusivement sur une source d'actualités, ne comprenaient que des titres.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And also there was an overlap in the borrowings that appear in the training set and the test set.\n",
            "Target: Et il y avait aussi un chevauchement dans les emprunts qui apparaissent dans l’ensemble de formation et l’ensemble de test.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This prevented the assessment of whether the modeling approach could actually generalize to previously unseen borrowings.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cela a empêché d’évaluer si l’approche de modélisation pouvait effectivement se généraliser aux emprunts précédemment invisibles.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So what we aim is to tackle some of these limitations in the task.\n",
            "Target: Nous visons donc à nous attaquer à certaines de ces limites dans la tâche.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: So to begin we, to begin with, we created a new dataset.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Alors, pour commencer, nous avons créé de nouvelles données.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ah the aim at a new dataset that was annotated with lexical borrowings and the aim was to create a test set that was as difficult as possible.\n",
            "Target: Avec de nouvelles données qui ont été annotées avec des emprunts lexicaux, le but était de créer un ensemble de test aussi difficile que possible.\n",
            "BLEU score: 8.593409002268001e-232\n",
            "\n",
            "Source: \n",
            "Target: So there would be minimal overlap in words and topics between the training set and test set.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il y aurait donc un chevauchement minimal entre les mots et les sujets entre l’ensemble de formation et l’ensemble de test.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And as a result, well, the test set comes from sources and dates that we're not seeing in the training set.\n",
            "Target: Et par conséquent, eh bien, l’ensemble de test provient de sources et de dates que nous ne voyons pas dans l’ensemble de formation.\n",
            "BLEU score: 1.0948551819675659e-231\n",
            "\n",
            "Source: \n",
            "Target: Here you can see that there's no overlap in the in the time.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ici, vous pouvez voir qu’il n’y a pas de chevauchement dans le temps.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: It's also, the test set is also very borrowing-dense.\n",
            "Target: L’ensemble de test est aussi très dense en termes d’emprunt.\n",
            "BLEU score: 1.0244914152188952e-231\n",
            "\n",
            "Source: \n",
            "Target: Just to give you some numbers, if the training set contains six borrowings per each thousand tokens, the test set contained twenty borrowings per each thousand tokens.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Juste pour vous donner quelques chiffres, si l’ensemble de formation contient six emprunts pour mille gages, l’ensemble de test contenait vingt emprunts pour mille gages.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The test set contained as many out of vocabulary words as possible.\n",
            "Target: L’ensemble de test contenait autant de mots hors-vocabulaire que possible.\n",
            "BLEU score: 9.97486269044271e-232\n",
            "\n",
            "Source: \n",
            "Target: In fact, ninety two percent of the borrowings in the test set are OOV.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En effet, quatre-vingt-douze pour cent des emprunts dans l’ensemble de test sont des OOV.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So, they were not seen during training.\n",
            "Target: Ils n’ont donc pas été vus pendant la formation.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: And the corpus consisted basically of a collection of texts that came from different sources of Spanish newspapers.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et le corpus consistait essentiellement en une collection de textes provenant de différentes sources de journaux espagnols.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And ah it was annotated by hand ah using two tags.\n",
            "Target: Et il a été annoté à la main en utilisant deux étiquettes.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: One for English lexical borrowings which is the majority of lexical borrowings in Spanish, and then the label other for borrowings from other languages.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Une pour les emprunts lexicaux anglais, constituant la majorité des emprunts lexicaux en espagnol, puis l’autre étiquette pour les emprunts d’autres langues.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We use CONLL formats and we used BIO encoding so that we could encode ah single token borrowings such as app or multi token borrowings such as machine learning.\n",
            "Target: Nous utilisons les formats CoNLL et avons utilisé l’encodage BIO pour pouvoir encoder des emprunts de gages simples tels que app ou des emprunts de gages multiples tels que l'apprentissage automatique.\n",
            "BLEU score: 1.016127520387007e-231\n",
            "\n",
            "Source: \n",
            "Target: These are the numbers of the corpus.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ce sont les chiffres du corpus.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: As you can see, it amounts to roughly three hundred seventy thousand tokens.\n",
            "Target: Comme vous pouvez le voir, il s’élève à environ trois cent soixante-dix mille gages.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: And here you have the number of spans that were labeled as English and the spans that were labeled as other borrowings and how many of them were unique.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et ici, vous avez le nombre d’étendages qui ont été étiquetés comme anglais et les étendages qui ont été étiquetés comme autres emprunts, et combien d’entre eux étaient uniques.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And here you have a couple of examples of the of the set of the dataset.\n",
            "Target: Et ici, vous avez quelques exemples de l’ensemble des données.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: As you can see for instance here, we have ah in the first example, we have the borrowing batch cooking which is a multi word borrowing.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comme vous pouvez le voir ici, par exemple, nous avons dans le premier exemple le batch cooking emprunteur qui est un emprunt de mots multiples.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And we have annotated it using the BIO um encode.\n",
            "Target: Et nous l’avons annoté en utilisant l’encodage BIO.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: So the BIO was used for words in Spanish so not for words that were not borrowed.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Donc le BIO a été utilisé pour des mots en espagnol, et non pour des mots qui n’ont pas été empruntés.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And here in this second example, you have benching and crash which are also labeled as borrowings from English.\n",
            "Target: Et ici, dans ce deuxième exemple, vous avez benching et crash qui sont également étiquetés comme des emprunts de l’anglais.\n",
            "BLEU score: 1.0244914152188952e-231\n",
            "\n",
            "Source: \n",
            "Target: So, once we had the dataset, we explored several models for the task of extracting and detecting these lexical borrowings.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Donc, une fois que nous avons eu les données, nous avons exploré plusieurs modèles pour la tâche d’extraction et de détection de ces emprunts lexicaux.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The first one that we tried was the conditional random field model.\n",
            "Target: Le premier que nous avons essayé était le modèle de champ aléatoire conditionnel.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Ah, this was the model that had been used on previous work.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ah, c’était le modèle qui avait été utilisé sur le travail antérieur.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And we used the same handcrafted features from that from those from that work.\n",
            "Target: Et nous avons utilisé les mêmes fonctions faites main à partir de celles de ce travail.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: As you can see, these are the features.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comme vous pouvez le voir, voici les fonctions.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: These are binary features such as the word or the token in upper case?\n",
            "Target: Ce sont des fonctions binaires telles que le mot ou le gage en majuscules ?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Is it title titlecase?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Est-ce une casse de titre ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Is it a quotation mark?\n",
            "Target: Est-ce un guillemet ?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Things like that, which are the type of features that one would expect in a named entity recognition task.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Des choses comme ça, qui sont le type de fonctions que l’on attendrait d’une tâche de named entity recognition.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: These are the results that we got.\n",
            "Target: Voici les résultats que nous avons obtenus.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We obtain fifty five F1 score using the the CRF model with handcrafted features.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous obtenons cinquante-cinq points en F1 en utilisant le modèle CRF avec des fonctions faites main.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Which is a huge different difference um compared to the reported F1 score of eighty six, which was the result obtained with the same CRF model, same features but on a different dataset also for Spanish lexical borrowing detection.\n",
            "Target: Ce qui est une énorme différence comparée au score F1 de quatre-vingt-six rapporté, qui était le résultat obtenu avec le même modèle CRF, les mêmes fonctions mais sur des données différentes également pour la détection de l’emprunt lexical espagnol.\n",
            "BLEU score: 9.594503055152632e-232\n",
            "\n",
            "Source: \n",
            "Target: So, this proves that the dataset that we created is more difficult and that we needed to explore more sophisticated models for these tasks.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Donc, cela prouve que les données que nous avons créées sont plus difficiles et que nous devions explorer des modèles plus sophistiqués pour ces tâches.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So, we tested two transformer based models.\n",
            "Target: Nous avons donc testé deux modèles basés sur la conversion.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We used BETO which is a monolingual BERT model trained for Spanish and also multilingual BERT.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous avons utilisé BETO qui est un modèle de Représentations d'encodeurs bidirectionnels à partir de transformateurs monolingue formé pour l’espagnol, mais aussi des Représentations d'encodeurs bidirectionnels à partir de transformateurs multilingues.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Both models we use them through the transformers library by HuggingFace.\n",
            "Target: Nous utilisons les deux modèles à travers la bibliothèque de conversion d’HuggingFace.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: These are the results that we got.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Voici les résultats que nous avons obtenus.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: As you can see, multilingual BERT performs better than BETO both on the development set and on the test set and across all metrics.\n",
            "Target: Comme vous pouvez le voir, les Représentations d'encodeurs bidirectionnels à partir de transformateurs multilingues fonctionnent mieux que les BETO à la fois sur l’ensemble de développement et sur l’ensemble de test, et à travers tous les indicateurs.\n",
            "BLEU score: 7.386826398032373e-232\n",
            "\n",
            "Source: \n",
            "Target: Just so we have ah an idea to compare, the CRF model obtained an eighty two.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Juste pour que nous ayons une idée à comparer, le modèle CRF a obtenu un quatre-vingt-deux.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The CRF model obtained a fifty five obtained a fifty five F1 score, whereas the multilingual BERT obtained eighty two, which is a big difference.\n",
            "Target: Le modèle CRF a obtenu cinquante-cinq points en F1, tandis que les Représentations d'encodeurs bidirectionnels à partir de transformateurs multilingues ont obtenu quatre-vingt-deux, ce qui est une grande différence.\n",
            "BLEU score: 9.336117803135294e-232\n",
            "\n",
            "Source: \n",
            "Target: So, once that we had those results, we asked ourselves another question which is, could we find a BiLSTM-CRF model, feed it with different types of embeddings, embeddings that encode different types of linguistic information and perform outperform the results obtained by transformer based models?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Donc, une fois que nous avons eu ces résultats, nous nous sommes posés une autre question qui est : pourrions-nous trouver un modèle BiLSTM-CRF, l’alimenter avec différents types d’intégrations, des intégrations qui encodent différents types d’informations linguistiques, et dépasser les résultats obtenus par les modèles basés sur la conversion ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So in order to do so, we ran some preliminary experiments, we we run this by BiLSTM-CRF model using flare library.\n",
            "Target: Donc, pour ce faire, nous avons effectué quelques expériences préliminaires ; nous avons exécuté ceci par le biais du modèle BiLSTM-CRF en utilisant la bibliothèque Flare.\n",
            "BLEU score: 8.06798322521923e-232\n",
            "\n",
            "Source: \n",
            "Target: And we tried experimented with different type of embeddings like transformer-based but also fast-text, character embeddings, and so on.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et nous avons essayé d’expérimenter différents types d’intégrations comme celles basées sur la conversion, mais aussi des intégrations de caractères, de texte rapide et ainsi de suite.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: What we found out was that transformer-based embeddings performed better than non contextualized embeddings, that the combination of English BERT and Spanish BETO embeddings outperform multilingual BERT embeddings.\n",
            "Target: Ce que nous avons découvert, c’est que les intégrations basées sur la conversion ont obtenu de meilleurs résultats que les intégrations non contextualisées, et que la combinaison d’intégrations de Représentations d'encodeurs bidirectionnels à partir de transformateurs en anglais et de BETO en espagnol dépasse les intégrations de Représentations d'encodeurs bidirectionnels à partir de transformateurs multilingues.\n",
            "BLEU score: 7.955640502424632e-232\n",
            "\n",
            "Source: \n",
            "Target: And that BPE embeddings produced better F1 and character embeddings produce better recall.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et aussi, que les intégrations de BPE produisent un meilleur F1 et les intégrations de caractères, un meilleur rappel.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: With that in mind, these were the best performing results that we got.\n",
            "Target: Dans cet esprit, ce sont les meilleurs résultats que nous avons obtenus.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Both models were BiLSTM-CRF models using flare.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les deux modèles étaient des modèles BiLSTM-CRF utilisant Flare.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: One was fed with BETO and BERT embeddings and BPE, and the other one BETO and BERT embeddings and BPE and also character embeddings.\n",
            "Target: L’un a été alimenté avec des intégrations de BETO, de Représentations d'encodeurs bidirectionnels à partir de transformateurs et de BPE, et l’autre avec des intégrations de BETO, de Représentations d'encodeurs bidirectionnels à partir de transformateurs, de BPE et aussi des intégrations de caractères.\n",
            "BLEU score: 8.460552185460498e-232\n",
            "\n",
            "Source: \n",
            "Target: This last one was the one that produced the highest F1 score on the test set, although the highest score on the development set was obtained by the one without character embeddings.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ce dernier était celui qui a produit le score F1 le plus élevé sur l’ensemble de test, bien que le score le plus élevé sur l’ensemble de développement ait été obtenu par celui sans intégrations de caractères.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Just ah to bear in mind that the best result that we got with multilingual BERT obtained an F1 of seventy six on the development set and eighty two on the test set.\n",
            "Target: Gardons juste à l’esprit que le meilleur résultat que nous avons obtenu avec Représentations d'encodeurs bidirectionnels à partir de transformateurs multilingues était un F1 de soixante-seize sur l’ensemble de développement et quatre-vingt-deux sur l’ensemble de test.\n",
            "BLEU score: 7.437597952034396e-232\n",
            "\n",
            "Source: \n",
            "Target: So this is an improvement compared to those results.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: C’est donc une amélioration comparée à ces résultats.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Finally, we asked ourselves another question which was can lexical borrowing detection be framed as transfer learning from language identification in code switching?\n",
            "Target: Enfin, nous nous sommes posés une autre question qui était de savoir si la détection d’emprunt lexical pouvait être encadrée comme apprentissage par transfert de l’identification de langue dans l’alternance codique ?\n",
            "BLEU score: 9.109159947227211e-232\n",
            "\n",
            "Source: \n",
            "Target: So, we run the same BiLSTM-CRF model that we had run using flare, but instead of using these unadapted transformer-based BETO and BERT embeddings, we used code switch embeddings.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous exécutons alors le même modèle BiLSTM-CRF que nous avions exécuté en utilisant Flare, mais au lieu d’utiliser ces intégrations de BETO et Représentations d'encodeurs bidirectionnels à partir de transformateurs basées sur la conversion non adaptées, nous avons utilisé les intégrations d’alternance codique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: What are code switch embeddings?\n",
            "Target: Qu’est-ce que les intégrations d’alternance codique ?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Well these are um embeddings that are have been fine tuned transformer-based embeddings that have been pretrained for language identification on the Spanish English section of the LinCE code switching dataset.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Eh bien, ce sont des intégrations qui ont été des intégrations basées sur la conversion ajustée, qui ont été préformées pour l’identification de langue sur la section anglaise espagnole des données d’alternance codique LinCE.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: LinCE is a dataset on code switching that has a section on Spanish English, Spanish English code switching.\n",
            "Target: LinCE est un ensemble de données sur l’alternance codique qui comporte une section sur l’anglais espagnol, l’alternance codique anglais espagnol.\n",
            "BLEU score: 1.0244914152188952e-231\n",
            "\n",
            "Source: \n",
            "Target: So we fed our BiLSTM-CRF with code switch embeddings and optionally character embeddings, BPE embeddings and so on.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous avons donc alimenté notre BiLSTM-CRF avec des intégrations d’alternance codique et éventuellement des intégrations de caractères, des intégrations de BPE et ainsi de suite.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The best result that we got was eighty four point twenty two, which is the highest across all the models that we tried on the test set.\n",
            "Target: Le meilleur résultat que nous avons obtenu était quatre-vingt-quatre points vingt-deux, ce qui est le plus élevé parmi tous les modèles que nous avons essayés sur l’ensemble de test.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Although the best result F1 score that we got on the development set, which was seventy nine, was lower than the best result obtained by the BiLSTM-CRF fed with unadapted embeddings.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Bien que le meilleur score F1 que nous ayons obtenu sur l’ensemble de développement, qui était de soixante-dix-neuf, était inférieur au meilleur résultat obtenu par le BiLSTM-CRF alimenté avec des intégrations non adaptées.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So, some conclusions from our work.\n",
            "Target: Voilà donc les conclusions de notre travail.\n",
            "BLEU score: 1.1200407237786664e-231\n",
            "\n",
            "Source: \n",
            "Target: We have ah we have produced a new dataset of Spanish newswire that is annotated with unassimilated lexical borrowings.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous avons produit de nouvelles données de fil d’actualité espagnol qui sont annotées avec des emprunts lexicaux non assimilés.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: This dataset is more borrowing dense and OOV-rich than previous resources.\n",
            "Target: Ces données sont plus denses en matière d’emprunt et riches en OOV par rapport aux ressources antérieures.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We have explored four types of models for lexical borrowing detection.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous avons exploré quatre types de modèles pour la détection d’emprunt lexical.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Um.\n",
            "Target: Hum.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: In terms of error analysis, well, recall was a weak point for all models.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En termes d’analyse des erreurs, eh bien, le rappel était un point faible pour tous les modèles.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ah, as you can see here, some frequent false negatives include uppercase borrowings, words that exist in both English and Spanish, for instance.\n",
            "Target: Ah, comme vous pouvez le voir ici, certains faux négatifs fréquents incluent des emprunts en majuscules, des mots qui existent à la fois en anglais et en espagnol, par exemple.\n",
            "BLEU score: 7.784451369270533e-232\n",
            "\n",
            "Source: \n",
            "Target: Also interestingly, BPE embeddings seem to improve F1 score.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il est également intéressant de noter que les intégrations de BPE semblent améliorer le score F1.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And character embedding seem to improve recall.\n",
            "Target: Et l’intégration de caractères semble améliorer le rappel.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Which ah it's an interesting finding that perhaps we can explore on future work.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ce qui est une découverte intéressante que peut-être nous pouvons explorer sur les travaux futurs.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Um.\n",
            "Target: Hum.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Well, this is everything that I have.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Eh bien, c’est tout ce que j’ai.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Thank you so much for listening.\n",
            "Target: Merci beaucoup pour votre écoute.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: My name is Antoine.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Je m’appelle Antoine.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: I'm a PhD student at the University of Massachusetts Amherst.\n",
            "Target: Je suis doctorant à l’Université du Massachusetts à Amherst.\n",
            "BLEU score: 1.1193096620723278e-231\n",
            "\n",
            "Source: \n",
            "Target: I am presenting our paper KinyaBERT: a Morphology-aware Kinyarwanda Language Model.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Je vous présente notre article KinyaBERT : un modèle de langue en kinyarwanda conscient de la morphologie.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Today, I'll talk about the motivation for this research.\n",
            "Target: Aujourd’hui, je vais parler de la motivation pour ces recherches.\n",
            "BLEU score: 1.0244914152188952e-231\n",
            "\n",
            "Source: \n",
            "Target: Then I'll present KinyaBERT model architecture in detail.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ensuite, je présenterai en détail l’architecture du modèle KinyaBERT.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: I'll then talk about our experimental results, then finish with some conclusions.\n",
            "Target: Je parlerai ensuite de nos résultats expérimentaux, puis je terminerai par quelques conclusions.\n",
            "BLEU score: 9.594503055152632e-232\n",
            "\n",
            "Source: \n",
            "Target: We all know that recent natural language processing advances have been made possible by the use of pretrained language models such as BERT.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous savons tous que les progrès récents du traitement du langage naturel ont été rendus possibles par l’utilisation de modèles de langues préformées tels que les Représentations d'encodeurs bidirectionnels à partir de transformateurs.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: However, there are still a number of limitations.\n",
            "Target: Cependant, il y a encore un certain nombre de limitations.\n",
            "BLEU score: 1.2183324802375697e-231\n",
            "\n",
            "Source: \n",
            "Target: Due to the complex morphology that is expressed by most morphologically rich languages, the ubiquitous byte pair encoding tokenization algorithm that I used cannot extract the exact subword lexical units, meaning the morphemes, which are needed for effective representation.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En raison de la morphologie complexe qui est exprimée par la plupart des langues morphologiquement riches, l’algorithme de marquage byte pair encoding omniprésent que j’ai utilisé ne peut pas extraire les unités lexicales sous-mots exactes, signifiant les morphèmes, qui sont nécessaires pour une représentation efficace.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For example, here we have three Kinyarwanda words that have several morphemes in them, but the BPE algorithms cannot extract them.\n",
            "Target: Par exemple, ici, nous avons trois mots en kinyarwanda ayant plusieurs morphèmes en eux, mais les algorithmes de BPE ne peuvent pas les extraire.\n",
            "BLEU score: 8.231055179516831e-232\n",
            "\n",
            "Source: \n",
            "Target: This is because some morphological rules produce different surface forms that hide the exact lexical information, and BPE, which is solely based on the surface forms, does not have access to this lexical model.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En effet, certaines règles morphologiques produisent différentes formes de surface qui cachent l’information lexicale exacte, et le BPE, qui est uniquement basé sur les formes de surface, n’a pas accès à ce modèle lexical.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The second challenge is that even if one had access to an oracle morphological analyzer, replacing BPE tokens with morphemes is not enough to express the morphological compositionality.\n",
            "Target: Le deuxième défi est que même si l’on avait accès à un analyseur morphologique oracle, remplacer les gages de BPE par des morphèmes n’est pas suffisant pour exprimer la compositionnalité morphologique.\n",
            "BLEU score: 7.720899511627474e-232\n",
            "\n",
            "Source: \n",
            "Target: A third gap in the research is that new pretrained language models are most often evaluated on high resource languages.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Une troisième lacune dans les recherches est que les nouveaux modèles de langue préformée sont le plus souvent évalués sur des langues à ressources élevées.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And we need to assess their applicability on low resources and diverse languages as well.\n",
            "Target: Et nous devons évaluer leur applicabilité sur des ressources faibles et diverses langues également.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Therefore, we present KinyaBERT, which is a simple but effective adaptation of the BERT architecture that is meant to more effectively handle morphologically rich languages.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par conséquent, nous présentons KinyaBERT, qui est une adaptation simple mais efficace de l’architecture des Représentations d'encodeurs bidirectionnels à partir de transformateurs destinée à gérer plus efficacement les langues morphologiquement riches.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We evaluate KinyaBERT on Kinyarwanda, a low resource morphologically rich language, which is spoken by more than twelve million people across Eastern and Central Africa.\n",
            "Target: Nous évaluons KinyaBERT sur le kinyarwanda, une langue low resource riche morphologiquement, qui est parlée par plus de douze millions de personnes à travers l’Afrique de l’Est et centrale.\n",
            "BLEU score: 3.677591797349584e-155\n",
            "\n",
            "Source: \n",
            "Target: The input to the model is either a sentence or a document.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: La saisie du modèle est soit une phrase, soit un document.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For example here, we have John twarahamubonye biradutangaza, which means we were surprised to find John there.\n",
            "Target: Par exemple, ici, nous avons John twarahamubonye biradutangaza, qui signifie « nous avons été surpris de trouver John là-bas ».\n",
            "BLEU score: 2.258627331321157e-78\n",
            "\n",
            "Source: \n",
            "Target: As you can see, Kinyarwanda words contains several morphemes that contain different information in them.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comme vous pouvez le voir, les mots en kinyarwanda comprennent plusieurs morphèmes qui renferment différentes informations.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Therefore, in our model, we pass this sentence or a document to a morphological analyzer.\n",
            "Target: Par conséquent, dans notre modèle, nous faisons passer cette phrase ou un document à un analyseur morphologique.\n",
            "BLEU score: 8.972141065609098e-232\n",
            "\n",
            "Source: \n",
            "Target: Which then generates morphemes contained in each of the words.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ce qui engendre alors des morphèmes contenus dans chacun des mots.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The morphemes usually are made of the stem and zero or more affixes.\n",
            "Target: Les morphèmes sont généralement constitués du radical et de zéro ou plus d’affixes.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The affixes may indicate tense, aspect, subject or object in verbs, and more often relates to the Bantu noun class for subjects and objects.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les affixes peuvent indiquer le temps, l’aspect, le sujet ou l’objet dans les verbes, et se rapportent plus souvent à la classe nom bantoue pour les sujets et les objets.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The morphological analyzer also produces a part of speech tag for each of the words.\n",
            "Target: L’analyseur morphologique produit également une étiquette de partie de discours pour chacun des mots.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: After this step, we make embeddings for the spee- for the part of speech tags.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Après cette étape, nous faisons des intégrations pour le dis- pour les étiquettes de la partie de discours.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Embeddings for the affixes.\n",
            "Target: Intégrations pour les affixes.\n",
            "BLEU score: 1.2882297539194154e-231\n",
            "\n",
            "Source: \n",
            "Target: And embeddings for the stem.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et intégrations pour le radical.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: These are the morphology level, these are the morphology level embeddings.\n",
            "Target: Il s’agit du niveau morphologique ; il s’agit des intégrations de niveau morphologique.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We then pass these embeddings through a morphology encoder, which is a small transformer encoder that is applied to each word independently.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous faisons ensuite passer ces intégrations à travers un encodeur morphologique, qui est un petit encodeur de conversion appliqué à chaque mot indépendamment.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The output of the are the vectors that are contextualized with the morphological information at each word.\n",
            "Target: Les résultats sont les vecteurs contextualisés avec les informations morphologiques à chaque mot.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Now, we perform composition where the morphological embeddings corresponding to part of speech and stem are concatenated together.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Maintenant, nous effectuons une composition où les intégrations morphologiques correspondant à une partie de discours et au radical sont concaténées ensemble.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We further concat we further concatenate them with another stem embedding at the sentence level.\n",
            "Target: Nous les concaténons en outre avec une autre intégration du radical au niveau de la phrase.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Then we form an input to the main sentence or document encoder.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ensuite, nous formons une saisie à la phrase principale ou l’encodeur de document.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The final output are contextualized embeddings that can be used for downstream NLP tasks.\n",
            "Target: Le résultat final donne des intégrations contextualisées qui peuvent être utilisées pour les tâches de TAL traitement automatique du langage naturel downstream.\n",
            "BLEU score: 8.412065649527267e-232\n",
            "\n",
            "Source: \n",
            "Target: For a morphological analyzer, we use finite state two level morphology principles with custom implementation that is tailored to the Kinyarwanda language.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Pour un analyseur morphologique, nous utilisons des principes de morphologie à deux niveaux à états finis avec une mise en œuvre personnalisée adaptée à la langue kinyarwanda.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We effectively model the morphology of all Kinyarwanda words, including verbals, nouns, demonstrative and possessive pronouns, numerals, and others.\n",
            "Target: Nous modélisons efficacement la morphologie de tous les mots en kinyarwanda, y compris les verbes, les noms, les pronoms démonstratifs et possessifs, les chiffres et autres.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We use an unsupervised part of speech tagging algorithm.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous utilisons une partie non supervisée d’algorithme de classification de discours.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: A first order factored model is used to account for morphology probability, basically the probability that is assigned by the morphological analyzer.\n",
            "Target: Un modèle factorisé de premier ordre est utilisé pour rendre compte de la probabilité morphologique, essentiellement la probabilité attribuée par l’analyseur morphologique.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We also take into consideration the part of speech tag precedence as well as the syntactic agreements that are present in the in the input words.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous prenons également en considération la priorité de l’étiquette de la partie de discours ainsi que les accords syntaxiques qui sont présents dans les mots de saisie.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The part of speech tagger uses a bidi bidirectional inference which improves upon the more often used Viterbi algorithm for decoding.\n",
            "Target: Le marqueur de partie de discours utilise une inférence bidirectionnelle bidi qui améliore le plus souvent l’algorithme Viterbi utilisé pour le décodage.\n",
            "BLEU score: 1.0003688322288243e-231\n",
            "\n",
            "Source: \n",
            "Target: A few remarks here for positional encoding.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Quelques remarques ici pour l'encodage positionnel.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: One, the morphology encoder does not use any positional encoding.\n",
            "Target: Premièrement, l’encodeur morphologique n’utilise aucun encodage positionnel.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This is because each of the morphemes occupies a known slot in the morphological model.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: C’est parce que chacun des morphèmes occupe un emplacement connu dans le modèle morphologique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Therefore, positional information is inherent when the morphemes are given.\n",
            "Target: Par conséquent, l’information positionnelle est inhérente lorsque les morphèmes sont donnés.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Second, the sentence encoder uses the so-called untied relative positional embeddings, which have been recently published at ICLR conference.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Deuxièmement, l’encodeur de phrase utilise les intégrations positionnelles relatives dites non liées, qui ont été récemment publiées lors de la conférence ICLR.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: This positional embeddings essentially disentangles positional correlations from token to token attention computation.\n",
            "Target: Ces intégrations positionnelles démêlent essentiellement les corrélations positionnelles de calcul d’attention gage à gage.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Similar to BERT, we use a masked language model pre-training objective.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: De manière similaire aux Représentations d'encodeurs bidirectionnels à partir de transformateurs, nous utilisons un objectif de préformation de modèle de langue masqué.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Essentially we have to predict both the stem and the affixes that are associated with the words.\n",
            "Target: Essentiellement, nous devons prévenir à la fois le radical et les affixes qui sont associés aux mots.\n",
            "BLEU score: 8.972141065609098e-232\n",
            "\n",
            "Source: \n",
            "Target: During pre-training, fifteen percent of all words are considered for prediction, of which eighty percent are masked, ten percent are swapped with random words, and ten percent are left unchanged.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Pendant la préformation, quinze pour cent de tous les mots sont considérés pour la prévention, dont quatre-vingt pour cent sont masqués, dix pour cent sont échangés avec des mots aléatoires et dix pour cent sont laissés inchangés.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For affix prediction, we face some multi label classification problem.\n",
            "Target: Pour la prévention d’affixe, nous faisons face à un problème de classification multi-étiquettes.\n",
            "BLEU score: 1.1409851298103347e-231\n",
            "\n",
            "Source: \n",
            "Target: For this, we either group together affixes into a fixed number of sets and predict the set as a class label.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Pour cela, nous regroupons les affixes ensemble dans un nombre fixe d’ensembles et prévenons l’ensemble comme une étiquette de classe.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The other option is to predict the affix probability vector.\n",
            "Target: Ou bien, l’autre option est de prévenir le vecteur de probabilité d’affixe.\n",
            "BLEU score: 9.788429383461836e-232\n",
            "\n",
            "Source: \n",
            "Target: We evaluate both of these approaches in our experiments.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous évaluons ces deux approches dans nos expériences.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We pre-train KinyaBERT on about two and half gigabytes of Kinyarwanda text, and compare it to three baseline models.\n",
            "Target: Nous préformons KinyaBERT sur environ deux giga-octets et demi de texte kinyarwanda, et le comparons à trois modèles de base.\n",
            "BLEU score: 8.614911585158347e-232\n",
            "\n",
            "Source: \n",
            "Target: One is a multilingual model called XLM-R, that is trained on a large text corpora that is made of multiple languages.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: L’un est un modèle multilingue appelé XLM-R, qui est formé sur un grand corpus de texte composé de plusieurs langues.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The other two baselines are pretrained on the same Kinyarwanda text using either the byte pair encoding algorithm or using morphological analysis without using the two tier transformer encoder architecture.\n",
            "Target: Les deux autres bases sont préformées sur le même texte kinyarwanda en utilisant soit l’algorithme de byte pair encoding, soit l’analyse morphologique sans utiliser l’architecture encodeur de conversion à deux niveaux.\n",
            "BLEU score: 3.2122473350116486e-155\n",
            "\n",
            "Source: \n",
            "Target: All models are configured in the base architecture, which is about between a hundred and a hundred and ten million parameters, with Kinyarwanda with KinyaBERT using the least number of parameters.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Tous les modèles sont configurés dans l’architecture de base, qui est d’environ cent à cent et dix millions de paramètres, avec le kinyarwanda avec KinyaBERT utilisant le plus petit nombre de paramètres.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: All models except the multilingual are pretrained for thirty two thousand gradient updates with a batch size of two thousand five hundred and sixty sequences in each batch.\n",
            "Target: Tous les modèles sauf les multilingues sont préformés pour trente-deux mille mises à jour de pentes avec une taille de lot de deux mille cinq cent soixante séquences dans chaque lot.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We evaluate the pretrained models on three sets of tasks.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous évaluons les modèles préformés sur trois ensembles de tâches.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: One is the GLUE benchmark which has often been used for evaluating the effectiveness of pretrained language models.\n",
            "Target: L’une est la référence GLUE qui a souvent été utilisée pour évaluer l’efficacité des modèles de langue préformée.\n",
            "BLEU score: 8.844844403089351e-232\n",
            "\n",
            "Source: \n",
            "Target: We obtain our GLUE benchmark data by translating the original benchmark data into Kinyarwanda using Google Translate.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous obtenons nos données de référence GLUE en traduisant les données de référence originales en kinyarwanda à l’aide de Google Translate.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The second task is Kinyarwanda named entity recognition benchmark, which is a high quality dataset that was annotated by trained native speakers.\n",
            "Target: La deuxième tâche est la référence de reconnaissance d'entité nommée kinyarwanda, qui est un ensemble de données de haute qualité ayant été annoté par des locuteurs natifs formés.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The third one is a news categorization task where we pull news articles from several websites and collecting their categorization tags that were assigned by the authors and then essentially trying to predict the same, the the same categories.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: La troisième est une tâche de catégorisation des actualités où nous extrayons des articles de actualités de plusieurs sites web et collectons leurs étiquettes de catégorisation qui ont été attribuées par les auteurs, puis essayons essentiellement de prévenir ces dernières, les mêmes catégories.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And now we go to the results.\n",
            "Target: Et maintenant, passons aux résultats.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: For the GLUE benchmark, we find that KinyaBERT consistently outperforms baseline models.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Pour la référence GLUE, nous constatons que KinyaBERT dépasse systématiquement les modèles de référence.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Here we show the average performance for ten finetuning runs.\n",
            "Target: Ici, nous montrons la performance moyenne pour dix cycles de raffinement.\n",
            "BLEU score: 1.0003688322288243e-231\n",
            "\n",
            "Source: \n",
            "Target: We also run a user evaluation of the translations that are produced by Google Translate.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous effectuons également une évaluation des traductions produites par Google Translate.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Essentially, user users rated about six thousand examples, assigning scores on a scale from one to four, assessing the quality of the translations.\n",
            "Target: Essentiellement, les utilisateurs ont évalué environ six mille exemples, en attribuant des scores sur une échelle de un à quatre et en évaluant la qualité des traductions.\n",
            "BLEU score: 9.50440384721771e-232\n",
            "\n",
            "Source: \n",
            "Target: The result is that many translations were noisy.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Le résultat est que beaucoup de traductions étaient bruyantes.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: But, all models had to cope with the same translation noise, and the relative performance between the models is still important to notice.\n",
            "Target: Mais tous les modèles ont dû faire face au même bruit de traduction, et la performance relative entre les modèles est toujours importante à remarquer.\n",
            "BLEU score: 9.689041594391036e-232\n",
            "\n",
            "Source: \n",
            "Target: For the named entity recognition task, we also find that KinyaBERT gives the best performance with the affix distribution regression variant performing best.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Pour la tâche de reconnaissance d'entité nommée, nous constatons également que KinyaBERT fournit les meilleures performances avec la variante de régression de distribution d’affixes fonctionnant le mieux.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: These results are also averages of ten finetuning runs.\n",
            "Target: Ces résultats sont également des moyennes de dix cycles de raffinement.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: For the news categorization task, we find mixed results.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Pour la tâche de catégorisation des actualités, nous trouvons des résultats mitigés.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Previous work on text classification for Kinyarwanda had found that simple keyword detection is mostly enough for solving this specific task.\n",
            "Target: Le travail antérieur sur la classification de texte pour le kinyarwanda avait trouvé que la détection de mot-clé simple est surtout suffisante pour résoudre cette tâche spécifique.\n",
            "BLEU score: 9.50440384721771e-232\n",
            "\n",
            "Source: \n",
            "Target: Therefore, there is less gain from using pretrained language models.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par conséquent, il y a moins d'avantage à utiliser des modèles de langue préformée.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: On this particular task of news categorization.\n",
            "Target: Sur cette tâche particulière de catégorisation des actualités.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We also conducted an ablation study to see if there are alternative structures that improve performance.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous avons également mené une étude d’ablation pour voir s’il existe des structures alternatives qui améliorent les performances.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For the GLUE benchmark, we find that using affix sets consistently performs better, while affix probability regression objective yields the best performance on named entity recognition.\n",
            "Target: Pour la référence GLUE, nous constatons que l’utilisation d’ensembles d’affixes donne constamment de meilleurs résultats, tandis que l’objectif de régression de probabilité d’affixe fournit les meilleures performances sur la reconnaissance d'entité nommée.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Also by looking at the low scores for finetuning, we find that KinyaBERT has better convergence in most cases.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: De plus, en examinant les faibles scores pour le raffinement, nous constatons que KinyaBERT a une meilleure convergence dans la plupart des cas.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So to conclude, this work has demonstrated the effectiveness of explicitly using morphological information in pretrained language models.\n",
            "Target: Donc, pour conclure, ce travail a démontré l’efficacité de l’utilisation explicite des informations morphologiques dans les modèles de langue préformée.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The proposed two tier transformer encoder architecture enables capturing morphological complexity morphological compositionality, which is an important aspect of morphologically rich languages.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: L’architecture à deux niveaux proposée permet de capturer la compositionnalité morphologique de la complexité morphologique, qui est un aspect important des langues morphologiquement riches.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: These findings should motivate further research into morphology aware language pretrained language models.\n",
            "Target: Ces résultats devraient motiver davantage les recherches sur les modèles de langue préformée conscients de la morphologie.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Hello, my name is Michał Pietruszka and it is my pleasure to present to you the paper titled Sparsifying Transformer Models with Trainable Representation Pooling.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Bonjour, je m’appelle Michal Pietruszka et j’ai le plaisir de vous présenter l’article intitulé Modèles de conversion parcimonieux avec la mise en commun de la représentation adaptative.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: A work done at Applica AI in cooperation with Lukasz Borchmann and Lukasz Garncarek.\n",
            "Target: Un travail réalisé à Applica intelligence artificielle en coopération avec Lukasz Borchmann et Lukasz Garncarek.\n",
            "BLEU score: 6.968148412761692e-155\n",
            "\n",
            "Source: \n",
            "Target: Let me start with the problems our work targets.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Permettez-moi de commencer par les problèmes que nous visons dans le cadre de notre travail.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Our method works well for the cases where long inputs are considered.\n",
            "Target: Notre méthode fonctionne bien pour les cas où de longues saisies sont considérées.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Roughly speaking, it is meant for the task orders and input of over two thousand tokens and the targets are shorter than the provided inputs.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En gros, c’est pour les ordres de tâche et de saisie de plus de deux mille gages, et les cibles sont plus courtes que les saisies fournies.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: This has some specific applications in NLP.\n",
            "Target: Cela a des applications spécifiques en TAL traitement automatique du langage naturel.\n",
            "BLEU score: 9.788429383461836e-232\n",
            "\n",
            "Source: \n",
            "Target: For example, one can imagine that given a long document, there's a need to summarize it, classify, answer the question about it, extract information or some key phrases.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, on peut imaginer qu’étant donné qu’un document est long, il est nécessaire de le résumer, de classer, de répondre à la question à ce sujet et d’extraire des informations ou certaines expressions clés.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Let me recall the vanilla transformer and our and its issue of its attention complexity that depends on the square of the input line.\n",
            "Target: Permettez-moi de rappeler la conversion vanille et sa question de sa complexité d’attention qui dépend du carré de la ligne de saisie.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: In the vanilla transformer, with full attention connectivity, relations of each token to every other token have to be calculated.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Dans la conversion vanille, avec une connectivité de pleine attention, les relations de chaque gage à chaque autre gage doivent être calculées.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The computational complexity of attention, this depends on the number of layers l, sequence length n, another sequence length, and the dimensionality of representations.\n",
            "Target: La complexité informatique de l’attention, qui dépend du nombre de couches l, de la longueur de séquence n, d’une autre longueur de séquence et de la dimensionnalité des représentations.\n",
            "BLEU score: 9.336117803135294e-232\n",
            "\n",
            "Source: \n",
            "Target: Similarly, in the decoder's cross attention, to this picture on the right side, the only difference here is that the target tokens are attending to the input tokens in this case.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: De même, dans l’attention croisée du décodeur, à cette image sur le côté droit, la seule différence ici est que les gages cibles sont attentifs aux gages de saisie dans ce cas.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Which can be seen also in this formula.\n",
            "Target: Ce que l’on retrouve également dans cette formule.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The BLEU score represents relations that have to be calculated.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Le score BLEU représente les relations qui doivent être calculées.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In case of the full attention, we need to calculate every relations within the input sequence.\n",
            "Target: Dans le cas de la pleine attention, nous devons calculer toutes les relations dans la séquence de saisie.\n",
            "BLEU score: 1.0518351895246305e-231\n",
            "\n",
            "Source: \n",
            "Target: Now, we see what happens when we have a blockwise encoder that works by limiting the tokens connectivity so that they can only see other nearby tokens.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Maintenant, nous voyons ce qui se passe lorsque nous avons un encodeur par bloc qui fonctionne en limitant la connectivité des gages afin qu’ils ne puissent voir que les autres gages à proximité.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The text is read in chunks which can drastically reduce the number of computations on the encoder side, but does not improve the decoder's cross attention as every input token is passed to the decoder anyway.\n",
            "Target: Le texte est lu en morceaux, ce qui peut réduire considérablement le nombre de calculs du côté de l’encodeur, mais n’améliore pas l’attention croisée du décodeur car chaque gage de saisie est de toute façon transmis au décodeur.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This method is often referred to as fusion in decoder.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cette méthode est souvent appelée fusion dans le décodeur.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The improvement here can be interpreted as changing one of the dependencies of n to another constant m representing the block size.\n",
            "Target: L’amélioration ici peut être interprétée comme changeant l’une des dépendances de n en une autre constante m représentant la taille du bloc.\n",
            "BLEU score: 1.0003688322288243e-231\n",
            "\n",
            "Source: \n",
            "Target: Our key observation is that most tokens are irrelevant for a wide variety of tasks and can be almost completely disregarded.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Notre observation clé est que la plupart des gages ne sont pas pertinents pour une grande variété de tâches et peuvent être presque complètement ignorés.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: This is exemplified on the slide.\n",
            "Target: Ceci est illustré sur la diapositive.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The only parts of the inputs are relevant to the desired output.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les seules parties des saisies sont pertinentes pour la sortie souhaitée.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For example.\n",
            "Target: Par exemple.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: One can read an article once marking the most important parts with a highlighter, and then produce a summary based on this part from the middle stage only.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: On peut lire un article une fois en marquant les parties les plus importantes avec un surligneur, puis produire un résumé basé sur cette partie à partir du stade intermédiaire seulement.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The cost of highlighting and deciding if the current token is essential to produce the summary is thus cheap and depends only on the token's representation.\n",
            "Target: Le coût de la mise en surbrillance et de la décision de savoir si le gage actuel est essentiel pour produire le résumé est donc peu élevé et ne dépend que de la représentation du gage.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The pooling of the highlighted tokens is possible.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: La mise en commun des gages en surbrillance est possible.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Thanks to our top k operator and its cost is negligible.\n",
            "Target: Grâce à notre meilleur opérateur k, son coût est négligeable.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The cost of producing a summary from a shortened input is also much lower than in the vanilla model when the whole input is considered.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Le coût de production d’un résumé à partir d’une saisie raccourcie est également beaucoup plus faible que dans le modèle vanille lorsque la saisie complète est prise en compte.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: But here's a question.\n",
            "Target: Mais une question se pose.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: How to select important tokens and backpropagate gradients to that selection?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comment sélectionner les gages importants et rétropropager les pentes vers cette sélection ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The essential underlying problem that we solve is to propose the trainable selection mechanism.\n",
            "Target: Le problème sous-jacent essentiel que nous résolvons est de proposer le mécanisme de sélection adaptatif.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: One that can allow for gradient to be back propagated during the training so that the network can learn to select the most important tokens.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Celui qui peut permettre à la pente de se rétropropager pendant la formation afin que le réseau puisse apprendre à sélectionner les gages les plus importants.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: More precisely Given some embeddings underscore obtained from a simple linear layer, the task is to return the highest scoring embeddings.\n",
            "Target: Plus précisément Compte tenu de certains soulignements d’intégrations obtenus à partir d’une couche linéaire simple, la tâche est de renvoyer les intégrations au score le plus élevé.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: First, the sequence is permuted and pairs are prepared so that the higher scoring vector is taken with the lower scoring one.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Tout d’abord, la séquence est permutée et les paires sont préparées de manière à ce que le vecteur de score le plus élevé soit pris avec le vecteur de score le plus faible.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Next, weights are calculated using boosted softmax over scores.\n",
            "Target: Ensuite, les poids sont calculés à l’aide de softmax boosté sur les scores.\n",
            "BLEU score: 1.1409851298103347e-231\n",
            "\n",
            "Source: \n",
            "Target: After each tournament round, new vectors and scores are composed as a linear combination of those pairs with the obtained weights.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Après chaque tour de tournoi, de nouveaux vecteurs et scores sont composés comme une combinaison linéaire de ces paires avec les poids obtenus.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So in short, we combine them linearly by performing a softmax over their scores.\n",
            "Target: Donc, en bref, nous les combinons linéairement en effectuant un softmax sur leurs scores.\n",
            "BLEU score: 1.1200407237786664e-231\n",
            "\n",
            "Source: \n",
            "Target: And while combining two tokens, some noise can be produces produced.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et tout en combinant deux gages, un certain bruit peut être produit.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: But it also allows the gradients to be propagated to all input embeddings.\n",
            "Target: Mais cela permet aussi de propager les pentes à toutes les intégrations de saisie.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: In short, a trainable top k we propose is based on performing a tournament like soft selection at each step.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En bref, un top k adaptatif que nous proposons est basé sur l’exécution d’un tournoi comme la sélection souple à chaque étape.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And from a different perspective, the representation pooling follows the encoder layer.\n",
            "Target: Et d’un point de vue différent, la mise en commun de la représentation suit la couche de l’encodeur.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: First, each representation is scored and then only those with the highest scores are passed to the next layer.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Tout d’abord, chaque représentation est notée, puis seules celles qui ont les scores les plus élevés sont passées à la couche suivante.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Encoding can be performed as in standard transformer architecture on the full length input.\n",
            "Target: L’encodage peut être effectué comme dans l’architecture de conversion standard sur la saisie pleine longueur.\n",
            "BLEU score: 9.257324954728539e-232\n",
            "\n",
            "Source: \n",
            "Target: It is however possible to process text in blocks of fixed length of fixed length and globally select the best representation.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il est cependant possible de traiter le texte par blocs de longueur fixe et de sélectionner globalement la meilleure représentation.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Here is an example of the representation pooling introduced after the encoder.\n",
            "Target: Voici un exemple de la mise en commun de représentation introduite après l’encodeur.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This directly influenced the cause of cross attention, which depends not on the input length N, but the constant K, representing the pooled length.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cela a directement influencé la cause de l’attention croisée, qui ne dépend pas de la longueur de saisie N, mais de la constante K, représentant la longueur mise en commun.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: This constant informs how many representations are selected and passed to the decoder.\n",
            "Target: Cette constante indique combien de représentations sont sélectionnées et transmises au décodeur.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Producing a summary from a shorter text is significantly cheaper than previous solution.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Produire un résumé à partir d’un texte plus court est nettement moins cher que la solution antérieure.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: As the sequence length can be shortened by a large factor.\n",
            "Target: Comme la longueur de séquence qui peut être raccourcie par un grand facteur.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: For example, we successfully used k of sixteen or even sixty times four or even sixty four times smaller than the value of n in our experiments.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, nous avons utilisé avec succès le k seize ou même soixante-quatre fois plus petit que la valeur de n dans nos expériences.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Please note that the beneficial impact of blockwise encoding and self attention is sustained.\n",
            "Target: Veuillez noter que l’impact bénéfique de l’encodage par blocs et de l’attention personnelle est maintenu.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Remember that the computational cost of attention depend on the square of the input length.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Rappelez-vous que le coût informatique de l’attention dépend du carré de la longueur de saisie.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Reducing it the input earlier during the encoding process can significantly lower the costs.\n",
            "Target: Réduire plus tôt la saisie pendant le processus d’encodage peut baisser considérablement les coûts.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: For the pyramidion model, we narrowed down the size of the representation on the output of each of each chosen layer, leading to the exponential reduction of computational cost as the encoding proceeds.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Pour le modèle pyramidion, nous avons réduit la taille de la représentation sur la sortie de chaque couche choisie, conduisant à la réduction exponentielle du coût informatique au fur et à mesure que l’encodage progresse.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: As you can see, the total computational cost of a full encoder here is less than two times the cost of the full-sized first layer.\n",
            "Target: Comme vous pouvez le voir, le coût informatique total d’un encodeur complet est ici moins de deux fois le coût de la première couche pleine grandeur.\n",
            "BLEU score: 8.06798322521923e-232\n",
            "\n",
            "Source: \n",
            "Target: When pooling is introduced earlier, the sum of all purple squares is thus bounded to a constant, not dependent on the number of layers l. But on the constant c, which can be influenced by the placing of the pooling layers within the network.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Lorsque la mise en commun est introduite plus tôt, la somme de tous les carrés violets est donc liée à une constante, qui ne dépend pas du nombre de couches l. Mais sur la constante c, qui peut être influencée par le placement des couches de mise en commun au sein du réseau.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Our improvements were benchmarked on eight thousand tokens long inputs.\n",
            "Target: Nos améliorations ont été évaluées sur la base de huit mille longues saisies de gages.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: And the figure shows that when pooling is engaged, the best scalability for the network's depth is achieved.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et la figure montre que lorsque la mise en commun est engagée, la meilleure évolutivité pour la profondeur du réseau est atteinte.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Here one can note that training the pyramidion of twenty four layers can be cheaper than training a two layer vanilla transformer on such long inputs.\n",
            "Target: Ici, on peut noter que la formation du pyramidion de vingt-quatre couches peut être moins chère que la formation d’une conversion vanille à deux couches sur des saisies aussi longues.\n",
            "BLEU score: 9.257324954728539e-232\n",
            "\n",
            "Source: \n",
            "Target: Not to mention how easily vanilla transformer can go out of memory for such a long input.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Sans parler de la facilité avec laquelle la conversion vanille peut perdre la mémoire pour une si longue saisie.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The qual quality qual qualitative comparison of our trend pyramidion to other baseline is performed on the long document summarization task, or given the body of an article from arXiv or PubMed, the task is to generate its abstract.\n",
            "Target: La comparaison qual qualité qual qualitative de notre pyramidion tendance à d’autres bases est effectuée sur la longue tâche de synthèse de document, ou compte tenu du corps d’un article d’arXiv ou PubMed, la tâche est de générer son résumé.\n",
            "BLEU score: 3.7147439042370906e-155\n",
            "\n",
            "Source: \n",
            "Target: Thus, one can see blockwise, which is our baseline, performs on the level of the re, recent state-of-the-art models, while the pyramidion retains or improves the performance of this competitive baseline.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ainsi, on peut voir bloc par bloc, qui est notre base, ce qui est effectué au niveau du re, les modèles ultra-modernes récents, tandis que le pyramidion conserve ou améliore la performance de cette base compétitive.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: At the same time, our model is eighty percent faster to train and over four hundred fifty percent faster at inference when compared to the blockwise baseline.\n",
            "Target: En même temps, notre modèle est quatre-vingts pour cent plus rapide à former et plus de quatre cent cinquante pour cent plus rapide à l’inférence quand il est comparé à la base par bloc.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Both models have much lower parameter counts and were trained from scratch on the chosen tasks.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les deux modèles ont un nombre de paramètres beaucoup plus faible et ont été formés à partir de zéro sur les tâches choisies.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Previous approaches to to achieve a similar performance had to use more parameters and leverage pretrained foundation foundational models and additional language pretraining objective to achieve similar performance.\n",
            "Target: Les approches antérieures visant à atteindre un rendement similaire devaient utiliser plus de paramètres et tirer parti des modèles fondamentaux de base préformés, ainsi que de l’objectif de préformation en langue supplémentaire pour atteindre un rendement similaire.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We invite you to read our full paper and use our GitHub code.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous vous invitons à lire notre article complet et à utiliser notre code GitHub.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Thank you for watching.\n",
            "Target: Merci d’avoir regardé cette vidéo.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Hello, this is Jiawei Zhou from Harvard University.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Bonjour, je m’appelle Jiawei Zhou de l’Université d’Harvard.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: I am very glad to present our work on Online Semantic Parsing for Latency Reduction in Task-Oriented Dialogue.\n",
            "Target: Je suis très heureux de présenter notre travail sur l’analyse syntaxique et sémantique en ligne pour la réduction de la latence dans le dialogue orienté sur la tâche.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This is joint work with Jason, Michael, Anthony and Sam from Microsoft Semantic Machines.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il s’agit d’un travail commun avec Jason, Michael, Anthony et Sam de Semantic Machines par Microsoft.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In task-oriented dialogue, a user interacts with the system that handles requests from user utterances usually in speaking.\n",
            "Target: Dans le dialogue orienté sur la tâche, un utilisateur interagit avec le système qui traite les discours de l’utilisateur énoncés généralement en parlant.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: From the finish of the user utterance to the system response there is often a noticeable delay.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: De la fin du discours de l’utilisateur à la réponse du système, il y a souvent un retard notable.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Under the hood, the user utterance is translated into an executable program.\n",
            "Target: Sous le capot, le discours de l’utilisateur est traduit en un programme exécutable.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Which is then executed so that the system can respond properly.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Celui-ci est ensuite exécuté afin que le système puisse répondre correctement.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Because the program is represented as a semantic graph that outlines the computation, where node represents a function invocation and its children are the arguments.\n",
            "Target: Parce que le programme est représenté comme un graphique sémantique qui décrit le calcul, où le nodule représente une invocation de fonction et ses enfants sont les arguments.\n",
            "BLEU score: 9.418382295637229e-232\n",
            "\n",
            "Source: \n",
            "Target: The great nodes mark instantaneous operations, but the others are slow to execute.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les grands nodules marquent des opérations instantanées, mais les autres mettent du temps à s’exécuter.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The simple example here we show, these programs can often be more complicated graphs beyond the tree structures.\n",
            "Target: Dans l’exemple simple que nous montrons ici, ces programmes peuvent souvent être des graphiques plus compliqués au-delà des structures arborescentes.\n",
            "BLEU score: 8.614911585158347e-232\n",
            "\n",
            "Source: \n",
            "Target: In this talk, we ask the question, can we start generating the program and executing it before the user even finishes the utterance so that the faster response can be achieved by the system?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Dans cette conférence, nous posons la question : pouvons-nous commencer à générer le programme et à l’exécuter avant même que l’utilisateur ait terminé le discours, afin que le système puisse obtenir une réponse plus rapide ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: This is the online prediction and decision problem.\n",
            "Target: C’est la prévention en ligne et le problème de décision.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: There are a lot of others in this realm.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il y en a beaucoup d’autres dans ce domaine.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Examples include simultaneous translation where a live interpreter translates one language to another in real time, smart text auto completion to guess the user intent, and Uber pool where the drivers are sent to where they might be needed based on the predicted demand.\n",
            "Target: Les exemples incluent la traduction simultanée où un interprète en direct traduit une langue en une autre en temps réel, la saisie automatique intelligente du texte pour deviner l’intention de l’utilisateur, et Uber pool où les chauffeurs sont envoyés là où ils pourraient être nécessaires en fonction de la demande prévue.\n",
            "BLEU score: 2.4962878878683494e-155\n",
            "\n",
            "Source: \n",
            "Target: All of these scenarios have one thing in common.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Tous ces scénarios ont une chose en commun.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: That is, it is beneficial to make decisions before seeing all the input.\n",
            "Target: C’est-à-dire qu’il est bénéfique de prendre des décisions avant de voir toutes les saisies.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: In our case, we are going to deal with online semantic parsing, which could be expected to be challenging as we have to guess what the user might say.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Dans notre cas, nous allons traiter de l’analyse syntaxique et sémantique en ligne, ce qui pourrait être difficile car nous devons deviner ce que l’utilisateur pourrait dire.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And it is also underexplored with no formal evaluation metric.\n",
            "Target: Et cela est également sous-exploré sans indicateur d’évaluation formel.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: First, let's look at how an ordinary system works.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Tout d’abord, regardons comment fonctionne un système ordinaire.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: It is operating offline by parsing to the program only at the end of the user utterance.\n",
            "Target: Il fonctionne hors ligne par l'analyse syntaxique du programme uniquement à la fin du discours de l’utilisateur.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Here, the character graph is predicted after seeing all the information.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ici, le graphique de caractères est prévenu après avoir vu toutes les informations.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In contrast, we are proposing an online system that compares at every utterance prefix.\n",
            "Target: En revanche, nous proposons un système en ligne qui compare à chaque préfixe de discours.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: For example, each time we see a new token, we predict a new graph.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, chaque fois que nous voyons un nouveau gage, nous prévenons un nouveau graphique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Notice that there could be errors.\n",
            "Target: Notez qu’il peut y avoir des erreurs.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: At the position of at the pool party with Barack Obama, we got a graph with the right nodes on the person and the event subject, but guess the wrong timing information.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: À l’emplacement de la pool party avec Barack Obama, nous avons obtenu un graphique avec les bons nodules sur la personne et le sujet d’activité, mais deviné les mauvaises informations de timing.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: This process goes on until we receive the full user utterance.\n",
            "Target: Ce processus se poursuit jusqu’à ce que nous recevions le discours complet de l’utilisateur.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: How would this affect the execution timeline in the offline system?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comment cela affecterait-il la chronologie d’exécution dans le système hors ligne ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We'll get the program graph at the end so that the system can start execution at this point.\n",
            "Target: Nous obtiendrons le graphique de programme à la fin afin que le système puisse commencer l’exécution à ce stade.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Remember that the great nodes are fast operations, so we only consider the execution timeline of the colored slow functions.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Rappelez-vous que les grands nodules sont des opérations rapides ; nous ne considérons donc que la chronologie d’exécution des fonctions lentes colorées.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: First, these two find person functions can be executed in parallel, highlighted in white from the pink box as they have no dependency on other functions.\n",
            "Target: Premièrement, ces deux fonctions Trouver une personne peuvent être exécutées en parallèle, surlignées en blanc à partir de la case rose car elles n’ont pas de dépendance à d’autres fonctions.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Next, the node create event can then get executed after obtaining results from lower level nodes and then the top function yield so the whole program is finished.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ensuite, l’activité de création de nodules peut ensuite être exécutée après avoir obtenu des résultats à partir de nodules de niveau inférieur, puis la fonction supérieure mène à l’achèvement de l’ensemble du programme.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The execution process is strict, restricted to the program dependency structure where some operations cannot be parallelized which induces a noticeable delay.\n",
            "Target: Le processus d’exécution est strict, limité à la structure de dépendance du programme où certaines opérations ne peuvent pas être parallélisées, ce qui induit un retard notable.\n",
            "BLEU score: 9.50440384721771e-232\n",
            "\n",
            "Source: \n",
            "Target: In our online system, where we predict as we go, the program execution can start earlier.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Dans notre système en ligne, où nous prévenons au fur et à mesure, l’exécution du programme peut commencer plus tôt.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Here, at the prefix after Obama we predict confidently that the find person function should be in the program, but the rest may contain errors as they are grayed out.\n",
            "Target: Ici, au préfixe après Obama, nous prévenons en toute confiance que la fonction Trouver une personne devrait être dans le programme, mais le reste peut contenir des erreurs car elles sont grisées.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The execution of the node can be immediately started as a step.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: L’exécution du nodule peut être immédiatement commencée comme une étape.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Then, with more tokens, we predict a totally new graph, but part of it has already being executed.\n",
            "Target: Ensuite, avec plus de gages, nous prévenons un graphique totalement nouveau, mais une partie est déjà en cours d’exécution.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: So, we only need to consider the rest of the nodes that we are confident about as well.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Donc, nous n’avons qu’à considérer le reste des nodules sur lesquels nous sommes confiants.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Here, another find person can be executed in parallel.\n",
            "Target: Ici, une autre fonction Trouver une personne peut être exécutée en parallèle.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Again, we may have wrong predictions.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Encore une fois, nous pouvons avoir des préventions erronées.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: With more text, we have more ability to make it right.\n",
            "Target: Avec plus de texte, nous avons plus de capacité à faire les choses correctement.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Such as the event time here where AM is also anticipated correctly.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comme l’heure de l’activité ici où AM est également anticipé correctement.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Then, we can start executing the rest following the program dependency structure.\n",
            "Target: Ensuite, nous pouvons commencer à exécuter le reste en suivant la structure de dépendance du programme.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: By overlapping the execution timeline with the utterance timeline, we save a big amount of time.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En chevauchant la chronologie d’exécution avec la chronologie du discours, nous gagnons beaucoup de temps.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So we proposed the task of online semantic parsing.\n",
            "Target: Nous avons donc proposé la tâche d’analyse syntaxique et sémantique en ligne.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: One underlying assumption is that the execution time dominates the model prediction time.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Une hypothèse sous-jacente est que le temps d’exécution domine le temps de prévention du modèle.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So we could only gain time by predicting earlier.\n",
            "Target: Donc, nous ne pouvions gagner du temps qu’en prévenant plus tôt.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Another assumption is that as the prediction and execution happen in the background, that it is not visible to users.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Une autre hypothèse est que, comme la prévention et l’exécution se produisent en arrière-plan, cela n’est pas visible par les utilisateurs.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: It is not necessary to maintain a consistent parsing history.\n",
            "Target: Il n’est pas nécessaire de maintenir un historique d’analyse syntaxique cohérent.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: So, we reparse from scratch after each token.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Donc, nous réanalysons à partir de zéro après chaque gage.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In particular, we propose a two step approach.\n",
            "Target: En particulier, nous proposons une approche en deux étapes.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: A proposed step that predicts a graph with complete structure and a select step that selects the nodes that are worth executing at this time.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Une étape proposée qui prévient un graphique avec une structure complète et une étape de sélection qui sélectionne les nodules valant la peine d’être exécutés pour le moment.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We had two variants of the proposed method.\n",
            "Target: Nous avions deux variantes de la méthode proposée.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: First approach combines a language model completion with full utterance to graph parsing.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: La première approche combine l’achèvement d’un modèle de langue avec un discours complet à analyse syntaxique de graphique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In particular, the prefix after Obama is first completed through a finetuned BART language model and then translated into a program with full offline parser.\n",
            "Target: En particulier, le préfixe après Obama est d’abord complété par un modèle de langue BART raffiné, puis traduit en un programme avec un analyseur hors ligne complet.\n",
            "BLEU score: 9.50440384721771e-232\n",
            "\n",
            "Source: \n",
            "Target: The second approach directly predicts the program from user utterance prefixes.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: La seconde approche prévient directement le programme à partir des préfixes de discours de l’utilisateur.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: This is achieved by training a single online parser to translate to the goal graph from each prefix.\n",
            "Target: Ceci est réalisé en formant un seul analyseur en ligne à traduire au graphique objectif à partir de chaque préfixe.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This facilitates the model to learn the right anticipation.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cela facilite au modèle l’apprentissage de la bonne anticipation.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In a bit more detail, how do we generate these graphs?\n",
            "Target: Avec un peu plus de détails, comment générons-nous ces graphiques ?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We formulate the problem by generating a serial version of the graph.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous formulons le problème en générant une version série du graphique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Each node or edge is represented by an action.\n",
            "Target: Chaque nodule ou arête est représenté(e) par une action.\n",
            "BLEU score: 1.0518351895246305e-231\n",
            "\n",
            "Source: \n",
            "Target: Here, we start from the first node.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ici, nous commençons par le premier nodule.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The number below records the absolute index in action history.\n",
            "Target: Le nombre ci-dessous enregistre l’indice absolu dans l’historique des actions.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Then, we got the second node.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ensuite, nous avons le deuxième nodule.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Next, is the edge between them.\n",
            "Target: Ensuite, il y a l’arête entre eux.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: It contains the pointer to the index of the previous node and the edge label.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il contient le pointeur vers l’indice du nodule antérieur et l’étiquette d’arête.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Zero here means connecting the most recent node with the node generated by the zeroth action and next node next edge.\n",
            "Target: Zéro signifie ici connecter le nodule le plus récent avec le nodule généré par l’action zéro, le nodule suivant et l’arête suivante.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This process goes on until we generate the full graph.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ce processus se poursuit jusqu’à ce que nous générions le graphique complet.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The underlying model is based on transformer with self pointing mechanism similar to a previous transition based parser.\n",
            "Target: Le modèle sous-jacent est basé sur un transformateur avec un mécanisme de pointage automatique similaire à un analyseur basé sur la transition antérieure.\n",
            "BLEU score: 8.319100378795605e-232\n",
            "\n",
            "Source: \n",
            "Target: After generating a complete graph, we obtained the action level probabilities that correspond to different parts of the graph.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Après avoir généré un graphique complet, nous avons obtenu les probabilités de niveau d’action qui correspondent aux différentes parties du graphique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We select confidence subgraphs based on the thresholding heuristic to be executed.\n",
            "Target: Nous sélectionnons des sous-graphiques de confiance en fonction de l’heuristique de seuillage à exécuter.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Later on, we're going to vary the threshold to achieve different tradeoffs between the latency reduction and the execution cost.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Plus tard, nous allons modifier le seuil pour obtenir différents compromis entre la réduction de la latence et le coût d’exécution.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For formal evaluation of the online methods, we propose final latency reduction or FLR metric.\n",
            "Target: Pour une évaluation formelle des méthodes en ligne, nous proposons une réduction finale de la latence ou un indicateur FLR.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Here's a recap of how an offline system finishes the execution timeline.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Voici un récapitulatif de la façon dont un système hors ligne termine la chronologie d’exécution.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In online systems, execution overlaps with the utterance timeline, so it ends earlier.\n",
            "Target: Dans les systèmes en ligne, l’exécution chevauche la chronologie de discours ; donc elle se termine plus tôt.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: FLR is defined as the reduction time compared to the offline system, marked by the end of the execution.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: FLR est défini comme le temps de réduction comparé au système hors ligne, marqué par la fin de l’exécution.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We conduct experiments on two large conversational semantic parsing datasets, SMCalFlow and TreeDST.\n",
            "Target: Nous menons des expériences sur deux grandes données d’analyse syntaxique et sémantique conversationnelle, SMCalFlow et TreeDST.\n",
            "BLEU score: 1.0832677820940877e-231\n",
            "\n",
            "Source: \n",
            "Target: Our graph based parser when operating offline, achieves state-of-the-art performance on parsing on both datasets.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Notre analyseur basé sur le graphique, lorsqu’il fonctionne hors ligne, atteint des performances de pointe sur l’analyse syntaxique sur les deux données.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The LM complete model also achieves nontrivial BLEU gain compared with the simple baseline of node completion.\n",
            "Target: Le modèle complet de LM réalise également un gain BLEU non trivial comparé à la simple base de l’achèvement du nodule.\n",
            "BLEU score: 1.2035620841904511e-231\n",
            "\n",
            "Source: \n",
            "Target: Now, let's look at the prediction accuracy of our prefix to graph parser.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Examinons maintenant la précision de prévention de notre préfixe pour l’analyseur de graphique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We test the match F1 score of graph tuples between the generation and the go graph in validation data in y axis for each prefix length in x axis represented by percentages.\n",
            "Target: Nous testons le score F1 de correspondance des tuples de graphique entre la génération et le graphique dans les données de validation dans l’axe y pour chaque longueur de préfixe, et dans l’axe x représenté par des pourcentages.\n",
            "BLEU score: 1.1484186507842885e-231\n",
            "\n",
            "Source: \n",
            "Target: Each of these curves represents a different model with the only difference in training data.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Chacune de ces courbes représente un modèle différent avec la seule différence de données de formation.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The bottom curve is the offline parser, and we mix in prefix data in different lengths to transition the model to an online parser.\n",
            "Target: La courbe du bas est l’analyseur hors ligne, et nous mélangeons les données de préfixe en différentes longueurs pour faire passer le modèle à un analyseur en ligne.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: For example, the legend prefix eighty percent plus means the model is trained with prefix data with prefix length larger than eighty percent of the full utterance length.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, le préfixe de légende quatre-vingts pour cent plus signifie que le modèle est formé avec les données de préfixe ayant une longueur de préfixe supérieure à quatre-vingts pour cent de la longueur totale du discours.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The upper left corner is the desired area.\n",
            "Target: Le coin supérieur gauche est la zone souhaitée.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: As we can see, the offline parser in black curve is not doing well on the prefix data.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comme nous pouvons le voir, l’analyseur hors ligne en courbe noire ne se porte pas bien sur les données de préfixe.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: As we're mixing more prefixes in training, the curve is lifting upper and left, performing better on all the prefix lengths.\n",
            "Target: Comme nous mélangeons plus de préfixes dans la formation, la courbe se soulève en haut à gauche, en étant plus performante sur toutes les longueurs de préfixe.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: However, the full utterance parsing performance is not affected in the upper right dot.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cependant, la performance complète d’analyse syntaxique du discours n’est pas affectée dans le point supérieur droit.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Based on these strong results, how much latency do we reduce?\n",
            "Target: Sur la base de ces résultats solides, combien de latence réduisons-nous ?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We measure the time by the number of source tokens and simulate different function execution times.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous mesurons le temps par le nombre de gages sources et simulons différents temps d’exécution de fonction.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The curves show the tradeoff between the FLR metric and the execution cost, measured by the number of excessive function costs that are not correct.\n",
            "Target: Les courbes montrent le compromis entre l’indicateur FLR et le coût d’exécution, mesuré par le nombre de coûts de fonction excessifs qui ne sont pas corrects.\n",
            "BLEU score: 8.06798322521923e-232\n",
            "\n",
            "Source: \n",
            "Target: This is achieved by varying the subgraph selection threshold.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ceci est réalisé en faisant varier le seuil de sélection du sous-graphique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: A higher threshold selects fewer functions of mistake, but obtains a smaller FLR, whereas the lower threshold more aggressively selects and executes programs.\n",
            "Target: Un seuil plus élevé sélectionne moins de fonctions d’erreur, mais obtient un FLR plus petit, tandis que le seuil inférieur sélectionne et exécute les programmes de manière plus agressive.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We compare the two approaches we propose and a baseline that does nothing but directly applying the offline parser for online use.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous comparons les deux approches que nous proposons et une base qui ne fait rien d’autre que d’appliquer directement l’analyseur hors ligne pour l’utilisation en ligne.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The upper left region is has the best FLR and cost tradeoff.\n",
            "Target: La région supérieure gauche a le meilleur compromis FLR et coût.\n",
            "BLEU score: 9.134374972545899e-232\n",
            "\n",
            "Source: \n",
            "Target: We see both of our methods beat the baseline by a large margin, and they perform more similarly on TreeDST.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous voyons nos deux méthodes battre la ligne de base par une grande marge, et elles fonctionnent de manière plus similaire sur TreeDST.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: While individual function execution is faster, there tends to be more run executions and lower latency reduction room.\n",
            "Target: Bien que l’exécution des fonctions individuelles soit plus rapide, cela tend à y avoir plus d’exécutions lancées et une marge de réduction de la latence plus faible.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: When individual function execution is slower, there is more room for FLR improvement.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Lorsque l’exécution des fonctions individuelles est plus lente, il y a plus de marge pour l’amélioration du FLR.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Our two approaches achieve better performance in different cost cost regions.\n",
            "Target: Nos deux approches permettent d’obtenir de meilleurs résultats dans différentes régions de coûts.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Overall, we achieve thirty to sixty three percent relative latency reduction depending on execution time and allowed cost.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Dans l’ensemble, nous obtenons une réduction de latence relative de trente à soixante-trois pour cent en fonction du temps d’exécution et du coût autorisé.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Finally, we have a breakdown of average latency reduction in tokens for each type of the function node when the allowed cost is three run executions.\n",
            "Target: Enfin, nous avons une répartition de la réduction de latence moyenne en gages pour chaque type de nodule de fonction lorsque le coût autorisé est de trois exécutions.\n",
            "BLEU score: 7.919883909890055e-232\n",
            "\n",
            "Source: \n",
            "Target: As we can see, there are gains all over the board.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comme nous pouvons le voir, il y a des gains partout.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: There are also some functions on which we gain impressive latency reduction where the red bar is much longer, such as find manager and recipient.\n",
            "Target: Il y a aussi certaines fonctions sur lesquelles nous obtenons une réduction de latence impressionnante où la barre rouge est beaucoup plus longue, comme trouver le gestionnaire et le destinataire.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: These are low level functions that do not have much dependency on others.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ce sont des fonctions de bas niveau qui n’ont pas beaucoup de dépendance des autres.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In conclusion, we proposed online semantic parsing as new task to explore with the rigorous latency reduction metric.\n",
            "Target: En conclusion, nous avons proposé une analyse syntaxique et sémantique en ligne comme nouvelle tâche à explorer avec l’indicateur rigoureux de réduction de la latence.\n",
            "BLEU score: 8.147480343967206e-232\n",
            "\n",
            "Source: \n",
            "Target: With a strong graph based semantic parser, we achieve relatively good latency reduction either through our pipeline approach with LM completion and a full parser or directly through a learned parser on the prefixes.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Avec un analyseur sémantique basé sur le graphique fort, nous obtenons une réduction de latence relativement bonne soit par notre approche pipeline avec l’achèvement du LM et un analyseur complet, soit directement par un analyseur appris sur les préfixes.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Moreover, our approach can be a general framework and can be applied to other executable semantic representations in different domains.\n",
            "Target: De plus, notre approche peut être un cadre général et être appliquée à d’autres représentations sémantiques exécutables dans différents domaines.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Future works could explore smarter prediction and execution integration method.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les travaux futurs pourraient explorer la méthode d’intégration de prévention et d’exécution plus intelligente.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Thanks for your listening.\n",
            "Target: Merci de votre attention.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Hi.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Bonjour.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: I'm going to discuss our work on generating retrieval augmented counterfactuals for question answering tasks.\n",
            "Target: Je vais discuter de notre travail sur la génération de contrefactuels améliorés d’extraction pour les tâches de réponse aux questions.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This is work done during my internship at Google Research, where I was mentored by Matthew Lamm and Ian Tenney.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: C’est le travail effectué lors de mon stage chez le centre de recherches Google, où j’ai été encadré par Matthew Lamm et Ian Tenney.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: To motivate the task, let me begin by defining a counterfactual.\n",
            "Target: Pour motiver la tâche, permettez-moi de commencer par définir un contrefactuel.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: In this work, we define a counterfactual as a perturbation of the input text that differs in some meaningful controlled way from the original text.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Dans ce travail, nous définissons un contrefactuel comme une perturbation du texte de saisie qui diffère d’une manière contrôlée significative du texte original.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And allows us to reason about the changes in the outcome or the task label.\n",
            "Target: Et cela nous permet de raisonner sur les changements dans le résultat ou l’étiquette de tâche.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: For instance, changing the words fascinating to captivating or expected to mind-numbing changes the sentiment for this movie review.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, changer les mots fascinants en captivants ou supposés abrutissants change le sentiment pour cette critique de film.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Similarly, adding the qualifier women's to the question changes the answer to the question in the example below.\n",
            "Target: De même, l’ajout du qualificatif féminin à la question modifie la réponse à la question dans l’exemple ci-dessous.\n",
            "BLEU score: 1.0518351895246305e-231\n",
            "\n",
            "Source: \n",
            "Target: Humans are typically robust to such perturbations compared to NLP models trained on the task.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les humains sont généralement robustes à de telles perturbations comparés aux modèles de TAL traitement automatique du langage naturel formés sur la tâche.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Why is that?\n",
            "Target: Pourquoi ça ?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The dataset may be sampled with systematic biases that lead to a simple decision boundary that is violated by the counterfactual.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les données peuvent être échantillonnées avec des biais systématiques qui conduisent à une limite de décision simple violée par le contrefactuel.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: As shown in this 2D classification problem.\n",
            "Target: Comme le montre ce problème de classification 2D.\n",
            "BLEU score: 1.0832677820940877e-231\n",
            "\n",
            "Source: \n",
            "Target: My work has found that adding counterfactual examples to the training data can make the model robust to such perturbations.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Mon travail a révélé que l’ajout d’exemples contrefactuels aux données de formation peut rendre le modèle robuste à de telles perturbations.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So, if counterfactuals are valuable, how can we generate them?\n",
            "Target: Donc, si les contrefactuels sont précieux, comment pouvons-nous les générer ?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This task is especially hard for NLP because here are three examples from three different NLP tasks.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cette tâche est particulièrement difficile pour le TAL traitement automatique du langage naturel car il y a ici trois exemples de trois tâches de TAL traitement automatique du langage naturel différentes.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: As you can see, examples that violate the decision boundary between outcomes need to be very carefully crafted by perturbing some attributes of the text that are underlined here.\n",
            "Target: Comme vous pouvez le voir, les exemples qui violent la limite de décision entre les résultats doivent être très soigneusement élaborés en perturbant certains attributs du texte qui sont soulignés ici.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This could be done by human annotation, but this is expensive and biased.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cela pourrait être fait par annotation civique, mais cela est coûteux et partial.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Some prior work has focused on using syntax trees or semantic role labeling.\n",
            "Target: Certains travaux antérieurs se sont concentrés sur l’utilisation d’arbres de syntaxe ou d'étiquetage de rôle sémantique.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: But the set of perturbations generated by these techniques are limited by the semantic framework.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Mais l’ensemble des perturbations générées par ces techniques sont limitées par le cadre sémantique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: More recent work has used masked language models to fill in masked portions of the text to change labels.\n",
            "Target: Des travaux plus récents ont utilisé des modèles de langue masqués pour remplir des parties masquées du texte afin de changer les étiquettes.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: But finding what parts of the text to perturb can be challenging.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Mais trouver quelles parties du texte perturber peut être difficile.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: There are more challenges to generating counterfactuals for question answering specifically.\n",
            "Target: Il y a plus de défis à générer des contrefactuels pour la réponse aux questions spécifiquement.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: This task requires background knowledge.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cette tâche nécessite des connaissances de base.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For instance, to perturb the original question is Indiana Jones Temple of Doom a prequel?\n",
            "Target: Par exemple, pour perturber la question originale : est-ce qu'Indiana Jones et le Temple maudit est une préquelle ?\n",
            "BLEU score: 1.1484186507842885e-231\n",
            "\n",
            "Source: \n",
            "Target: We need to be aware of the other movies in the franchise to get to a question like is Indiana Jones Raiders of the Lost Ark a prequel?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous devons être au courant des autres films de la franchise pour arriver à une question comme : est-ce qu’Indiana Jones et les Aventuriers de l'arche perdue est une préquelle ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Furthermore, random perturbations can lead to questions that are not answerable with the available evidence or have false premises.\n",
            "Target: En outre, des perturbations aléatoires peuvent conduire à des questions qui ne répondent pas avec les preuves disponibles ou ont de fausses prémisses.\n",
            "BLEU score: 9.893133360884868e-232\n",
            "\n",
            "Source: \n",
            "Target: Moreover, some question perturbations can lead to significant semantic drift from the original input.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: De plus, certaines perturbations de question peuvent conduire à une dérive sémantique significative par rapport à la saisie originale.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For instance, this question is Indiana Jones practicing child slavery in Temple of Doom?\n",
            "Target: Par exemple, est-ce qu’Indiana Jones pratique l’esclavage des enfants dans le Temple maudit ?\n",
            "BLEU score: 1.1200407237786664e-231\n",
            "\n",
            "Source: \n",
            "Target: We propose a very simple yet effective technique called retrieve generate filter or RGF, to tackle counterfactual perturbations of questions, and also aims to tackle all the other aforementioned challenges.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous proposons une technique très simple mais efficace appelée Retrieve Generate Filter ou RGF, pour s’attaquer aux perturbations contrefactuelles des questions, et qui vise également à relever tous les autres défis susmentionnés.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The core intuition behind RGF is that the necessary background information that is needed to generate perturbations may be present in the near misses made by a question answering model.\n",
            "Target: L’intuition de base derrière le RGF est que les informations d’arrière-plan nécessaires pour générer des perturbations peuvent être présentes dans les quasi-accidents causés par un modèle de réponse aux questions.\n",
            "BLEU score: 9.257324954728539e-232\n",
            "\n",
            "Source: \n",
            "Target: For instance, the state-of-the-art model REALM produces the following top k answers to the question who is the captain of the Richmond Football Club?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, le modèle REALM à la fine pointe de la technologie produit les premières réponses k suivantes à la question : qui est le capitaine du Richmond Football Club ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: While it does recover the original reference passage and answer Trent Cotchin as the top most choice.\n",
            "Target: Bien qu’il récupère le passage de référence original et répond Trent Cotchin en premier choix.\n",
            "BLEU score: 4.849829366726974e-155\n",
            "\n",
            "Source: \n",
            "Target: It also retrieves additional passages and answers which can be used to guide question perturbation.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il extrait également des passages et des réponses supplémentaires qui peuvent être utilisés pour guider la perturbation de question.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For instance, it recovers two more answers corresponding to the captains of the reserve team and the women's team of the same club, and this can lead to interesting edits.\n",
            "Target: Par exemple, il récupère deux autres réponses correspondant aux capitaines de l’équipe de réserve et de l’équipe féminine du même club, ce qui peut conduire à des modifications intéressantes.\n",
            "BLEU score: 7.58460821834181e-232\n",
            "\n",
            "Source: \n",
            "Target: To summarize, RGF first retrieves top k most relevant answers and contexts which don't match the reference answer in context.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Pour résumer, le RGF extrait d’abord les premières réponses k les plus pertinentes et les contextes qui ne correspondent pas à la réponse de référence dans le contexte.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Following this step, the question generation model conditions on these alternate answers to generate a question that corresponds to them.\n",
            "Target: Suite à cette étape, le modèle de production de questions conditionne ces réponses alternatives pour générer une question qui leur correspond.\n",
            "BLEU score: 8.510469113101058e-232\n",
            "\n",
            "Source: \n",
            "Target: And finally, we can filter the generated questions based on minimality or based on the type of semantic perturbation we are interested in introducing.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et enfin, nous pouvons filtrer les questions générées en fonction de la minimalité ou du type de perturbation sémantique que nous voulons introduire.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Going over each step in greater detail for retrieval, we use a retrieve then read model like REALM that takes as input the original question, and a large corpus like Wikipedia.\n",
            "Target: En passant en revue chaque étape plus en détail pour l’extraction, nous utilisons un modèle lu comme REALM qui prend comme saisie la question originale, et un grand corpus comme Wikipédia.\n",
            "BLEU score: 9.181748633447778e-232\n",
            "\n",
            "Source: \n",
            "Target: It consists of two modules.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Il est constitué de deux modules.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: The retriever module performs similarity search over a dense index of passages to retrieve the top k most relevant passages to the question.\n",
            "Target: Le module d’extraction effectue des recherches de similarité sur un indice dense de passages pour extraire les passages k les plus pertinents à la question.\n",
            "BLEU score: 1.2751495852793276e-231\n",
            "\n",
            "Source: \n",
            "Target: And a reader module then extracts a span from each passage as a potential answer.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Et un module de lecture extrait ensuite un étendage de chaque passage en tant que réponse potentielle.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: REALM retrieves the gold passage and answer in most cases.\n",
            "Target: REALM extrait le passage d’or et la réponse dans la plupart des cas.\n",
            "BLEU score: 1.1409851298103347e-231\n",
            "\n",
            "Source: \n",
            "Target: However, in this work, we are more interested in the answers and context that it retrieves further down the line.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cependant, dans ce travail, nous sommes plus intéressés par les réponses et le contexte qu’il extrait plus loin dans la ligne.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In the next step, question generation, we use these alternate answers and contexts to regenerate new questions that correspond to these alternatives.\n",
            "Target: Dans l’étape suivante, production de questions, nous utilisons ces réponses alternatives et contextes pour générer de nouvelles questions qui correspondent à ces alternatives.\n",
            "BLEU score: 9.893133360884868e-232\n",
            "\n",
            "Source: \n",
            "Target: Question generation model is a pre trained text-to-text transformer that is fine-tuned on the NQ data to generate a question for an answer that's marked in context.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Le modèle de production de questions est une conversion texte-à-texte préformé qui est raffiné sur les données NQ pour générer une question pour une réponse marquée dans le contexte.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: During inference we supply the question generation model, the alternative answer and context that we retrieved in the previous step.\n",
            "Target: Au cours de l’inférence, nous fournissons le modèle de production de questions, la réponse alternative et le contexte que nous avons extraits à l’étape antérieure.\n",
            "BLEU score: 8.147480343967206e-232\n",
            "\n",
            "Source: \n",
            "Target: For example, for the query who is the captain of the Richmond Football Club?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, pour la question : qui est le capitaine du Richmond Football Club ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: REALM retrieves passages about the club's women's team, captained by Jess Kennedy, and the question generation model generates the query who captained Richmond Football Club's first ever women's team?\n",
            "Target: REALM extrait des passages sur l’équipe féminine du club, dirigée par Jess Kennedy, et le modèle de production de questions génère la requête « qui a été le capitaine de la toute première équipe féminine du Richmond Football Club ? »\n",
            "BLEU score: 4.362712208760745e-155\n",
            "\n",
            "Source: \n",
            "Target: Which has a specific semantic perturbation.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Qui a une perturbation sémantique spécifique.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: In a similar fashion, we also get queries like who captained Richmond's VFL Reserve team?\n",
            "Target: D’une manière similaire, nous recevons également des questions comme : qui était capitaine de l’équipe de réserve VFL de Richmond ?\n",
            "BLEU score: 8.510469113101058e-232\n",
            "\n",
            "Source: \n",
            "Target: Or who did graham negate in the grand final last year?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ou qui Graham a-t-il invalidé lors de la grande finale l’année dernière ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Finally, we filter out a subset of the generated queries based on some desired characteristics.\n",
            "Target: Enfin, nous filtrons un sous-ensemble des requêtes générées en fonction de certaines caractéristiques souhaitées.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: As motivated earlier, we would like to ensure that the new question is still semantically close to the original.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comme motivé plus tôt, nous aimerions nous assurer que la nouvelle question est toujours sémantiquement proche de l’originale.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For filtering techniques that doesn't require additional supervision, we simply retain new questions that have a small token label edit distance from the original question.\n",
            "Target: Pour les techniques de filtrage qui ne nécessitent pas de supervision supplémentaire, nous conservons simplement de nouvelles questions qui ont une petite distance d’édition de l’étiquette de gage par rapport à la question d’origine.\n",
            "BLEU score: 9.929306298309508e-232\n",
            "\n",
            "Source: \n",
            "Target: For example, we remove the question who did graham negate in the grand final last year?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, nous supprimons la question : qui Graham a-t-il invalidé lors de la grande finale l’année dernière ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Because it has a longer edit distance from the original question.\n",
            "Target: Car elle a une distance d’édition plus longue par rapport à la question d’origine.\n",
            "BLEU score: 1.1200407237786664e-231\n",
            "\n",
            "Source: \n",
            "Target: In our experiments, we demonstrate that this simple heuristic can be used to augment and queue training data.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Dans nos expériences, nous démontrons que cette heuristique simple peut être utilisée pour améliorer et mettre en file d’attente les données de formation.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We also experiment with a filtering strategy that is based on the type of semantic perturbation.\n",
            "Target: Nous expérimentons également une stratégie de filtrage basée sur le type de perturbation sémantique.\n",
            "BLEU score: 8.164587463486296e-232\n",
            "\n",
            "Source: \n",
            "Target: To this end, we use a general purpose query decomposition framework called QED.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: À cette fin, nous utilisons un cadre de décomposition de requête à usage général appelé QED.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: QED identifies two parts to the question, a predicate and a reference.\n",
            "Target: QED identifie deux parties à la question, un prédicat et une référence.\n",
            "BLEU score: 1.1640469867513693e-231\n",
            "\n",
            "Source: \n",
            "Target: References are noun phrases in the question that correspond to entities in the context.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Les références sont des syntagmes nominaux dans la question qui correspondent à des entités dans le contexte.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: A predicate is basically the remaining portion of the question.\n",
            "Target: Un prédicat est essentiellement la partie restante de la question.\n",
            "BLEU score: 1.0244914152188952e-231\n",
            "\n",
            "Source: \n",
            "Target: For example, we are able to decompose the query who captained Richmond's first ever women's team into two references: Richmond Football Club women's team and the predicate who captained X. A model trained on reference predicate annotations for NQ gives us this question decomposition.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, nous sommes en mesure de décomposer la requête « qui a dirigé la toute première équipe féminine de Richmond en deux références » : l’équipe féminine du Richmond Football Club et le prédicat qui a dirigé X. Un modèle formé sur les annotations du prédicat de référence pour NQ nous fournit cette décomposition de question.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Decomposing both the original and generated question based on QED allows us to categorize our generated counterfactuals for evaluation.\n",
            "Target: La décomposition à la fois de la question originale et générée basée sur QED nous permet de catégoriser nos contrefactuels générés pour l’évaluation.\n",
            "BLEU score: 9.893133360884868e-232\n",
            "\n",
            "Source: \n",
            "Target: Specifically, we obtain two groups of questions.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Plus précisément, nous obtenons deux groupes de questions.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Those that undergo a reference change while retaining predicates, and those that undergo a predicate change and optionally add references.\n",
            "Target: Ceux qui subissent un changement de référence tout en conservant les prédicats, et ceux qui subissent un changement de prédicat et ajoutent éventuellement des références.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: For instance, who captained Richmond's VFL reserve team is a reference change?\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Par exemple, qui a été capitaine de l’équipe de réserve de Richmond VFL est-il un changement de référence ?\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: While, who wears number nine for the club is a predicate change.\n",
            "Target: Alors que, qui porte le numéro neuf pour le club est un changement de prédicat.\n",
            "BLEU score: 9.257324954728539e-232\n",
            "\n",
            "Source: \n",
            "Target: We now evaluate the effectiveness of RGF perturbations when augmented to training data.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous évaluons maintenant l’efficacité des perturbations RGF lorsqu’elles sont améliorées par rapport aux données de formation.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: So, to effectively evaluate the effectiveness of counterfactual augmentation in particular, we experiment with two strong data augmentation baselines.\n",
            "Target: Ainsi, pour évaluer efficacement l’efficacité de l’élargissement contrefactuel en particulier, nous expérimentons deux bases de forte amélioration de données.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: The first baseline, called random answer and question generation, adds data that has no relation with the original question.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: La première base, appelée réponse aléatoire et production de questions, ajoute des données qui n’ont pas de relation avec la question originale.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: That is, passages and answers are simply randomly sampled from Wikipedia.\n",
            "Target: Autrement dit, les passages et les réponses sont simplement échantillonnés au hasard à partir de Wikipédia.\n",
            "BLEU score: 9.109159947227211e-232\n",
            "\n",
            "Source: \n",
            "Target: This baseline basically adds more data that looks like NQ.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cette base ajoute essentiellement plus de données ressemblant à NQ.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: With the second baseline gold answer and question generation, we specifically update the retrieval portion of our method.\n",
            "Target: Avec la deuxième réponse d’or de base et la production de questions, nous mettons spécifiquement à jour la partie extraction de notre méthode.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: Here, alternate answers are just chosen from the same passage that contained the gold answer.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ici, les réponses alternatives sont simplement choisies à partir du même passage qui contenait la réponse d’or.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: How base how do the baselines and RGF ah augmentation perform on reading comprehension where the model has access to question and context?\n",
            "Target: Comment l’élargissement des bases et du RGF fonctionne sur la reading comprehension où le modèle a accès à la question et au contexte ?\n",
            "BLEU score: 4.3521358589488775e-155\n",
            "\n",
            "Source: \n",
            "Target: We experiment with six out of domain datasets and present results here, where data is the training data is doubled in augmentation.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous expérimentons avec six données hors domaine et présentons les résultats ici, où les données qui sont les training data sont doublées dans l’élargissement.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We find that both data augmentation baselines are not able to improve our domain generalization.\n",
            "Target: Nous constatons que les deux bases d’amélioration de données ne sont pas en mesure d’améliorer notre généralisation du domaine.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: In fact, an ensemble of six models trained on the original data seems to be the most competitive baseline.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En effet, un ensemble de six modèles formés sur les données originales semble être la base la plus compétitive.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Comparing against that baseline, we find that RGF counterfactuals are able to improve out of domain performance while maintaining in domain performance.\n",
            "Target: En comparant avec cette base, nous constatons que les contrefactuels RGF sont capables d’améliorer les performances hors domaine tout en maintenant les performances de domaine.\n",
            "BLEU score: 8.147480343967206e-232\n",
            "\n",
            "Source: \n",
            "Target: This suggests that filling in the reasoning gaps of the model via counterfactual augmentation is more effective than adding more data from the training distribution.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cela suggère que combler les lacunes de raisonnement du modèle via l’élargissement contrefactuel est plus efficace que d’ajouter plus de données de la distribution de formation.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Furthermore, we find that using retrieval to sample alternative outcomes or answers is important for effective CDA.\n",
            "Target: En outre, nous constatons que l’utilisation de l’extraction pour échantillonner d’autres résultats ou réponses est importante pour un CDA efficace.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: We also experiment with open domain QA setting where the model only sees the question and once again we evaluate on four out of domain datasets.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous expérimentons également un paramètre QA de domaine ouvert où le modèle ne voit que la question et une fois de plus, nous évaluons sur quatre données hors domaine.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We find that baseline models are not as effective for out of domain generalization.\n",
            "Target: Nous constatons que les modèles de référence ne sont pas aussi efficaces pour notre généralisation hors domaine.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: However, data augmentation with RGF shows more significant improvements.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cependant, l’amélioration de données avec RGF montre des améliorations plus significatives.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We even improve in the in domain NQ dataset.\n",
            "Target: Nous nous améliorons même dans les données NQ de domaine.\n",
            "BLEU score: 1.0244914152188952e-231\n",
            "\n",
            "Source: \n",
            "Target: We hypothesized that the counterfactual data augmentation aids the model in learning better query encodings for very similar queries.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous avons émis l’hypothèse que l’amélioration de données contrefactuelle aide le modèle à apprendre de meilleurs encodages de requête pour des requêtes très similaires.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Finally, we also evaluate on the model's ability to improve consistency in the local neighborhood of the original question.\n",
            "Target: Enfin, nous évaluons également la capacité du modèle à améliorer la cohérence dans le voisinage local de la question originale.\n",
            "BLEU score: 8.614911585158347e-232\n",
            "\n",
            "Source: \n",
            "Target: Consistency measures the proportion of questions correctly answered by the model where both the original and the counterfactual query are correctly answered.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: La cohérence mesure la proportion de questions correctement répondues par le modèle où à la fois la requête originale et celle contrefactuelle reçoivent une réponse correcte.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: This explicitly helps us to measure the model's robustness to small perturbations in the neighborhood of the original input.\n",
            "Target: Cela nous aide explicitement à mesurer la robustesse du modèle à de petites perturbations dans le voisinage de la saisie originale.\n",
            "BLEU score: 8.510469113101058e-232\n",
            "\n",
            "Source: \n",
            "Target: We experiment with five datasets which contain pairs of questions that are semantically close to each other.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Nous expérimentons avec cinq données qui contiennent des paires de questions sémantiquement proches les unes des autres.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Apart from the three datasets AQA, AmbigQA and QUOREF-Contrast set that are already available, we also evaluate on RGF counterfactuals that are paired with original NQ questions based on whether they underwent a predicate change or reference change.\n",
            "Target: Mis à part les trois données AQA, AmbigQA et QUOREF-Contrast définies qui sont déjà disponibles, nous évaluons également sur les contrefactuels RGF qui sont synchronisés avec les questions NQ originales selon qu'elles ont subi un changement de prédicat ou un changement de référence.\n",
            "BLEU score: 3.581216623897135e-155\n",
            "\n",
            "Source: \n",
            "Target: These subsets were annotated in-house to eliminate noise and are provided as a resource.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Ces sous-ensembles ont été annotés en interne pour éliminer le bruit et sont fournis en tant que ressource.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: All baselines are unable to significantly improve consistency with the ensemble model improving consistency by a small margin.\n",
            "Target: Toutes les bases sont incapables d’améliorer de manière significative la cohérence avec le modèle d’ensemble en améliorant la cohérence d’une petite marge.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: \n",
            "Target: However, RGF counterfactual augmentation has impressive gains in consistency both on prior datasets and the two subsets we curated for reference and predicate perturbations.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Cependant, l’élargissement contrefactuel du RGF a des gains impressionnants en cohérence à la fois sur les données antérieures et les deux sous-ensembles que nous avons sélectionnés pour les perturbations de référence et de prédicat.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: Note that the augmented RGF data is not biased by perturbation type, only the evaluation sets are.\n",
            "Target: Notez que les données RGF améliorées ne sont pas biaisées par le type de perturbation, seuls les ensembles d’évaluation le sont.\n",
            "BLEU score: 8.510469113101058e-232\n",
            "\n",
            "Source: \n",
            "Target: In fact, a qualitative inspection of the kinds of counterfactuals generated show that the generated questions contain several diverse perturbations.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: En effet, une inspection qualitative des types de contrefactuels générés montre que les questions générées contiennent plusieurs perturbations diverses.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For instance, this original question on the population of Walnut Grove, Minnesota is perturbed along different dimensions like town, state, country, and along different predicates like location, poverty, number of schools.\n",
            "Target: Par exemple, cette question originale sur la population de Walnut Grove, au Minnesota, est perturbée par différentes dimensions comme la ville, l’État, le pays, et par différents prédicats comme l’emplacement, la pauvreté et le nombre d’écoles.\n",
            "BLEU score: 3.743881382418666e-155\n",
            "\n",
            "Source: \n",
            "Target: Audio of perturbations are context specific.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: L’audio des perturbations est spécifique au contexte.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: For example, for this other question about the Wimbledon ah singles tournament, the perturbation is along type of game, type of tournament, or the game outcome.\n",
            "Target: Par exemple, pour cette autre question sur le tournoi en simple de Wimbledon, la perturbation est liée au type de jeu, au type de tournoi ou au résultat du jeu.\n",
            "BLEU score: 1.1008876702055895e-231\n",
            "\n",
            "Source: \n",
            "Target: Final takeaways; we tackle the task of counterfactual data augmentation and perturbations for information seeking queries and tackle its unique challenges via a reversal of the generation approach, over generate using near misses of the model and filter based on perturbation type or minimality.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Derniers points à retenir : nous abordons la tâche d’amélioration des données contrefactuelle et les perturbations pour les requêtes recherchant des informations et abordons ses défis uniques via un renversement de l’approche de génération, et générons plus en utilisant des quasi-accidents du modèle et un filtre basé sur le type de perturbation ou la minimalité.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: We find that this technique requires no additional supervision and the examples are labeled for augmentation.\n",
            "Target: Nous constatons que cette technique ne nécessite aucune supervision supplémentaire et les exemples sont étiquetés pour l’élargissement.\n",
            "BLEU score: 1.0669733992029681e-231\n",
            "\n",
            "Source: \n",
            "Target: Augmentation improves out of domain generalization and neighborhood consistency.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: L’élargissement améliore la généralisation de domaine et la cohérence du voisinage.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Source: And we find that RGF counterfactuals are semantically diverse without introducing bias during augmentation.\n",
            "Target: Et nous constatons que les contrefactuels RGF sont sémantiquement divers sans introduire de biais lors de l’élargissement.\n",
            "BLEU score: 8.972141065609098e-232\n",
            "\n",
            "Source: \n",
            "Target: Thank you.\n",
            "BLEU score: 0\n",
            "\n",
            "Source: Merci.\n",
            "Target: \n",
            "BLEU score: 0\n",
            "\n",
            "Average BLEU score for all sentence pairs: 5.735065111387928e-81\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def load_aligned_data(file_path):\n",
        "    aligned_sentences = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "        for i in range(0, len(lines) - 1, 2):\n",
        "            source = lines[i].strip()\n",
        "            target = lines[i+1].strip()\n",
        "            aligned_sentences.append((source, target))\n",
        "    return aligned_sentences\n",
        "\n",
        "\n",
        "def evaluate_alignment(aligned_sentences):\n",
        "    total_bleu = 0\n",
        "    count = 0\n",
        "    for source, target in aligned_sentences:\n",
        "\n",
        "        source_tokens = source.split()\n",
        "        target_tokens = target.split()\n",
        "\n",
        "\n",
        "        bleu_score = sentence_bleu([source_tokens], target_tokens)\n",
        "        total_bleu += bleu_score\n",
        "        count += 1\n",
        "        print(f\"Source: {source}\")\n",
        "        print(f\"Target: {target}\")\n",
        "        print(f\"BLEU score: {bleu_score}\\n\")\n",
        "\n",
        "\n",
        "    average_bleu = total_bleu / count if count > 0 else 0\n",
        "    print(f\"Average BLEU score for all sentence pairs: {average_bleu}\")\n",
        "    return average_bleu\n",
        "\n",
        "file_path = 'data/aligned_output.txt'\n",
        "aligned_sentences = load_aligned_data(file_path)\n",
        "average_bleu_score = evaluate_alignment(aligned_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBDVOBdrzjfc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
